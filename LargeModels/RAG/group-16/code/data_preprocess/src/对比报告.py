import json
import os
from datetime import datetime
from collections import defaultdict

# ========== 1. åŠ è½½ JSONL æ–‡ä»¶ ==========
def load_jsonl(jsonl_path):
    """åŠ è½½ä¸€ä¸ªJSONLæ–‡ä»¶"""
    corpus = []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            corpus.append(json.loads(line))
    print(f"âœ… åŠ è½½ {os.path.basename(jsonl_path)}ï¼š{len(corpus)} æ¡è®°å½•")
    return corpus


# ========== 2. åˆ†å—ç»Ÿè®¡ç±» ==========
class ChunkStatistics:
    def __init__(self):
        self.reset()

    def reset(self):
        self.total_chunks = 0
        self.chunk_lengths = []
        self.semantic_breaks = 0
        self.doc_chunk_counts = defaultdict(int)

    def add_chunk(self, doc_id, text):
        self.total_chunks += 1
        self.chunk_lengths.append(len(text))
        self.doc_chunk_counts[doc_id] += 1

        # ç®€æ˜“è¯­ä¹‰æ–­è£‚æ£€æµ‹ï¼ˆå¥æœ«éæ ‡ç‚¹ï¼‰
        if text.strip() and text.strip()[-1] not in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', '\n', '"']:
            self.semantic_breaks += 1

    def get_report(self, name):
        if not self.chunk_lengths:
            return f"âš ï¸ {name}: æ— æ•°æ®"

        avg_len = sum(self.chunk_lengths) / len(self.chunk_lengths)
        report = f"""
{'='*60}
ğŸ“˜ åˆ†å—ç­–ç•¥: {name}
{'='*60}
ğŸ“Š åŸºæœ¬ç»Ÿè®¡:
  - æ€»åˆ†å—æ•°: {self.total_chunks}
  - å¹³å‡å—é•¿åº¦: {avg_len:.1f} å­—ç¬¦
  - æœ€çŸ­å—: {min(self.chunk_lengths)} å­—ç¬¦
  - æœ€é•¿å—: {max(self.chunk_lengths)} å­—ç¬¦
  - åŸå§‹æ–‡æ¡£æ•°: {len(self.doc_chunk_counts)}
  - å¹³å‡æ¯æ–‡æ¡£å—æ•°: {self.total_chunks / len(self.doc_chunk_counts):.2f}

âš ï¸ è¯­ä¹‰å®Œæ•´æ€§:
  - ç–‘ä¼¼è¯­ä¹‰æ–­è£‚: {self.semantic_breaks} ä¸ª ({self.semantic_breaks/self.total_chunks*100:.1f}%)

ğŸ“ˆ é•¿åº¦åˆ†å¸ƒ:
  - 0-256å­—ç¬¦: {sum(1 for l in self.chunk_lengths if l <= 256)}
  - 257-512å­—ç¬¦: {sum(1 for l in self.chunk_lengths if 256 < l <= 512)}
  - 513-768å­—ç¬¦: {sum(1 for l in self.chunk_lengths if 512 < l <= 768)}
  - 769+å­—ç¬¦: {sum(1 for l in self.chunk_lengths if l > 768)}
{'='*60}
"""
        return report


# ========== 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š ==========
def compare_chunking(length_file, parent_child_file, output_path):
    # è¯»å–æ•°æ®
    length_corpus = load_jsonl(length_file)
    parent_child_corpus = load_jsonl(parent_child_file)

    # ç»Ÿè®¡é•¿åº¦åˆ†å—
    length_stats = ChunkStatistics()
    for doc in length_corpus:
        doc_id = doc.get("doc_id", "unknown")
        text = doc.get("content", "")
        length_stats.add_chunk(doc_id, text)

    # ç»Ÿè®¡çˆ¶å­åˆ†å—
    parent_child_stats = ChunkStatistics()
    for doc in parent_child_corpus:
        doc_id = doc.get("doc_id", "unknown")
        text = doc.get("child_content", "")
        parent_child_stats.add_chunk(doc_id, text)

    # æ±‡æ€»æŠ¥å‘Š
    report_lines = [
        "=" * 80,
        "åˆ†å—ç­–ç•¥å¯¹æ¯”æŠ¥å‘Š",
        f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "=" * 80,
        ""
    ]
    report_lines.append(length_stats.get_report("é•¿åº¦åˆ†å— (Length-based)"))
    report_lines.append(parent_child_stats.get_report("çˆ¶å­åˆ†å— (Parent-Child)"))

    # æ±‡æ€»è¡¨æ ¼å¯¹æ¯”
    report_lines.append("\n" + "=" * 80)
    report_lines.append("ğŸ“‹ ç­–ç•¥å¯¹æ¯”æ±‡æ€»")
    report_lines.append("=" * 80)
    report_lines.append("{:<20} {:<12} {:<15} {:<15}".format("ç­–ç•¥", "æ€»åˆ†å—æ•°", "å¹³å‡å—é•¿åº¦", "è¯­ä¹‰æ–­è£‚ç‡"))
    report_lines.append("-" * 80)
    for name, stats in [("é•¿åº¦åˆ†å—", length_stats), ("çˆ¶å­åˆ†å—", parent_child_stats)]:
        if stats.total_chunks > 0:
            avg_len = sum(stats.chunk_lengths) / len(stats.chunk_lengths)
            break_rate = stats.semantic_breaks / stats.total_chunks * 100
            report_lines.append("{:<20} {:<12} {:<15.1f} {:<15.1f}%".format(
                name, stats.total_chunks, avg_len, break_rate
            ))

    # å»ºè®®éƒ¨åˆ†
    report_lines.append("\n\nğŸ’¡ ç­–ç•¥é€‰æ‹©å»ºè®®:")
    report_lines.append("-" * 80)
    report_lines.append("â€¢ é•¿åº¦åˆ†å—: ç®€å•å¿«é€Ÿï¼Œé€‚åˆç»“æ„å‡åŒ€çš„æŠ€æœ¯æ–‡æ¡£ã€‚")
    report_lines.append("â€¢ çˆ¶å­åˆ†å—: ä¿ç•™ä¸Šä¸‹æ–‡å±‚çº§ï¼Œé€‚åˆå¤šå±‚æ ‡é¢˜æˆ–å¤æ‚æŠ¥å‘Šç±»æ–‡æ¡£ã€‚")
    report_lines.append("=" * 80)

    # è¾“å‡º
    report_text = "\n".join(report_lines)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(report_text)

    print(f"\nğŸ“„ åˆ†å—å¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}\n")
    print(report_text)


# ========== 4. ä¸»å‡½æ•°å…¥å£ ==========
if __name__ == "__main__":
    # é…ç½®è¾“å…¥è¾“å‡ºè·¯å¾„
    length_file = "chunked_corpus/howtocook/chunked_corpus_length.jsonl"        # æ›¿æ¢ä¸ºé•¿åº¦åˆ†å—ç»“æœè·¯å¾„
    parent_child_file = "chunked_corpus/howtocook/chunked_corpus.jsonl"  # æ›¿æ¢ä¸ºçˆ¶å­åˆ†å—ç»“æœè·¯å¾„
    output_report = "chunking_comparison_report.txt"

    compare_chunking(length_file, parent_child_file, output_report)
