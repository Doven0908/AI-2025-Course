import dspy
from dspy.teleprompt import BootstrapFewShot
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter
import time
import json
import re
from abc import ABC, abstractmethod
import requests
import warnings

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')


class DSPyPromptOptimizer:
    """DSPyæç¤ºè¯ä¼˜åŒ–å™¨ - ä¿®å¤å›¾ç‰‡ä¸­çš„é—®é¢˜"""

    def __init__(self, ollama_client):
        self.ollama = ollama_client
        self.optimization_history = []

    def _generate_extraction_template(self, question_text: str) -> str:
        """æ ¹æ®é—®é¢˜å†…å®¹åŠ¨æ€ç”Ÿæˆä¿¡æ¯æŠ½å–æ¨¡æ¿"""
        import re
        
        # ä»é—®é¢˜ä¸­æå–éœ€è¦æŠ½å–çš„ä¿¡æ¯ç±»å‹
        question_lower = question_text.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«äº§å“ç›¸å…³å…³é”®è¯
        if "äº§å“" in question_text and ("ä»·æ ¼" in question_text or "å”®ä»·" in question_text):
            template = (
                "è¯·ä»æ–‡æœ¬ä¸­æŠ½å–äº§å“åç§°å’Œä»·æ ¼ï¼Œå¹¶ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n"
                "äº§å“ï¼š<product>ï¼Œä»·æ ¼ï¼š<price>\n\n"
                "æ–‡æœ¬ï¼š{text}\n"
                "å›ç­”ï¼š"
            )
        # æ£€æŸ¥æ˜¯å¦åŒ…å«äººåã€åœ°ç‚¹ã€æ—¶é—´
        elif "äººå" in question_text or "åœ°ç‚¹" in question_text or "æ—¶é—´" in question_text:
            template = (
                "è¯·ä»æ–‡æœ¬ä¸­æŠ½å–äººåã€åœ°ç‚¹å’Œæ—¶é—´ï¼Œå¹¶ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n"
                "é‡è¦æç¤ºï¼šå¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªäººåï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼›å¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªåœ°ç‚¹ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼›å¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªæ—¶é—´ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ã€‚\n"
                "äººåï¼š<name1,name2,...>ï¼Œåœ°ç‚¹ï¼š<location1,location2,...>ï¼Œæ—¶é—´ï¼š<time1,time2,...>\n\n"
                "æ–‡æœ¬ï¼š{text}\n"
                "å›ç­”ï¼š"
            )
        # é»˜è®¤ä½¿ç”¨äººåã€åœ°ç‚¹ã€æ—¶é—´æ¨¡æ¿
        else:
            template = (
                "è¯·ä»æ–‡æœ¬ä¸­æŠ½å–äººåã€åœ°ç‚¹å’Œæ—¶é—´ï¼Œå¹¶ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n"
                "é‡è¦æç¤ºï¼šå¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªäººåï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼›å¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªåœ°ç‚¹ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼›å¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªæ—¶é—´ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ã€‚\n"
                "äººåï¼š<name1,name2,...>ï¼Œåœ°ç‚¹ï¼š<location1,location2,...>ï¼Œæ—¶é—´ï¼š<time1,time2,...>\n\n"
                "æ–‡æœ¬ï¼š{text}\n"
                "å›ç­”ï¼š"
            )
        
        return template

    def optimize_prompt(self, task_type: str, input_question: str,
                        strategies: List[str] = None, model_type: str = "local") -> Dict[str, Any]:
        """
        ä¼˜åŒ–æç¤ºè¯ - å®Œå…¨åŒ¹é…å›¾ç‰‡ä¸­çš„æ ¼å¼
        """
        try:
            print(f"ğŸ¯ å¼€å§‹ä¼˜åŒ–æç¤ºè¯")
            print(f"ğŸ“Š ä»»åŠ¡ç±»å‹: {task_type}")
            print(f"ğŸ’­ è¾“å…¥é—®é¢˜: {input_question}")

            # é»˜è®¤ç­–ç•¥ - åŒ¹é…å›¾ç‰‡ä¸­çš„é€‰é¡¹
            if strategies is None:
                strategies = ["zero_shot", "few_shot"]

            # æå–è¾“å…¥å†…å®¹ï¼ˆæ ¹æ®ä»»åŠ¡ç±»å‹æå–ä¸åŒçš„å†…å®¹ï¼‰
            comment_text = self._extract_input_text(input_question, task_type)

            # ç”Ÿæˆä¼˜åŒ–åçš„æç¤ºè¯ - å®Œå…¨åŒ¹é…å›¾ç‰‡æ ¼å¼
            optimized_prompt = self._generate_exact_prompt_for_ui(task_type, comment_text, strategies)

            # ä»»åŠ¡åˆ†æ - åŒ¹é…å›¾ç‰‡ä¸­çš„å­—å…¸æ ¼å¼ï¼ˆä¿®æ­£task_typeï¼‰
            task_analysis = self._generate_exact_task_analysis(task_type, comment_text)

            # å¤æ‚åº¦è¯„ä¼° - ä¿®å¤N/Aé—®é¢˜
            complexity = self._generate_complexity_assessment(comment_text)

            # è´¨é‡è¯„åˆ†ä¸å»ºè®®ï¼ˆå¯å‘å¼ï¼‰
            quality_score, improvement_suggestions = self._heuristic_quality_and_suggestions(
                task_type, comment_text, strategies
            )

            result = {
                "optimized_prompt": optimized_prompt,
                "task_analysis": task_analysis,  # ç›´æ¥è¿”å›å­—å…¸ï¼Œä¸è½¬å­—ç¬¦ä¸²
                "complexity": complexity,
                "complexity_level": complexity,  # ä¸ºä¸å‰ç«¯å…¼å®¹ï¼Œé‡å¤ä¸€ä»½
                "task_type": task_type,
                "strategies_used": strategies,
                "model_type": model_type,
                "quality_score": quality_score,
                "improvement_suggestions": improvement_suggestions,
                "status": "success"
            }

            # è®°å½•ä¼˜åŒ–å†å²
            self.optimization_history.append({
                "task_type": task_type,
                "input": input_question,
                "output": optimized_prompt,
                "timestamp": time.time(),
                "analysis": task_analysis
            })

            print(f"âœ… æç¤ºè¯ä¼˜åŒ–å®Œæˆ")
            return result

        except Exception as e:
            error_msg = f"æç¤ºè¯ä¼˜åŒ–é”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                "error": error_msg,
                "status": "error",
                "task_analysis": {"error": "åˆ†æå¤±è´¥"},
                "complexity": "N/A",
                "complexity_level": "N/A",
                "quality_score": 0.0,
                "improvement_suggestions": ["è¯·æ£€æŸ¥è¾“å…¥ä¸ä»»åŠ¡ç±»å‹æ˜¯å¦åŒ¹é…"]
            }

    def _extract_comment_text(self, input_question: str) -> str:
        """ä»é—®é¢˜ä¸­æå–è¯„è®ºå†…å®¹ - ç²¾ç¡®åŒ¹é…å›¾ç‰‡æ ¼å¼"""
        # åŒ¹é…å›¾ç‰‡ä¸­çš„ç²¾ç¡®æ ¼å¼ï¼šè¿™éƒ¨ç”µå½±çš„è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼Ÿè¯„è®ºï¼šâ€˜...â€™
        match = re.search(r"è¯„è®ºï¼šâ€˜([^']+)'", input_question)
        if match:
            return match.group(1)

        # å¤‡ç”¨åŒ¹é…æ ¼å¼
        match = re.search(r"è¯„è®ºï¼š'([^']+)'", input_question)
        if match:
            return match.group(1)

        return input_question

    def _extract_input_text(self, input_question: str, task_type: str) -> str:
        """æ ¹æ®ä»»åŠ¡ç±»å‹æå–è¾“å…¥æ–‡æœ¬"""
        if task_type in ["text_classification", "sentiment_analysis", "sentiment_classification"]:
            # æ–‡æœ¬åˆ†ç±»ï¼šæå–è¯„è®ºå†…å®¹
            return self._extract_comment_text(input_question)
        elif task_type == "information_extraction":
            # ä¿¡æ¯æŠ½å–ï¼šæå–æ–‡æœ¬éƒ¨åˆ†ï¼ˆå¯èƒ½åŒ…å«åœ¨å¼•å·ä¸­ï¼‰
            match = re.search(r"['""]([^'""]+)['""]", input_question)
            if match:
                return match.group(1)
            # å¦‚æœæ²¡æœ‰å¼•å·ï¼Œå°è¯•æå–"æå–"æˆ–"ä»"åé¢çš„å†…å®¹
            match = re.search(r"(?:æå–|ä»)[^ï¼š:]*[ï¼š:]([^ï¼Œã€‚ï¼Ÿ]+)", input_question)
            if match:
                return match.group(1).strip()
            return input_question
        elif task_type == "question_answering":
            # é—®ç­”ä»»åŠ¡ï¼šæå–é—®é¢˜éƒ¨åˆ†
            if "é—®é¢˜ï¼š" in input_question:
                match = re.search(r"é—®é¢˜ï¼š([^ï¼Œã€‚ï¼Ÿ]+)", input_question)
                if match:
                    return match.group(1).strip()
            # å¦‚æœåŒ…å«å¼•å·ï¼Œæå–å¼•å·å†…çš„å†…å®¹
            match = re.search(r"['""]([^'""]+)['""]", input_question)
            if match:
                return match.group(1)
            return input_question
        else:
            # å…¶ä»–ä»»åŠ¡ï¼šç›´æ¥è¿”å›æ•´ä¸ªè¾“å…¥
            return input_question

    def _generate_exact_prompt_for_ui(self, task_type: str, comment_text: str,
                                      strategies: List[str]) -> str:
        """ç”Ÿæˆå®Œå…¨åŒ¹é…å›¾ç‰‡æ ¼å¼çš„ä¼˜åŒ–æç¤ºè¯ - é’ˆå¯¹ä¸åŒä»»åŠ¡ç±»å‹ç”Ÿæˆå·®å¼‚åŒ–æ¨¡æ¿"""

        if task_type == "text_classification":
            return self._build_text_classification_prompt(comment_text, strategies)
        elif task_type == "information_extraction":
            return self._build_information_extraction_prompt(comment_text, strategies)
        elif task_type == "question_answering":
            return self._build_question_answering_prompt(comment_text, strategies)
        elif task_type in ["sentiment_analysis", "sentiment_classification"]:
            return self._build_text_classification_prompt(comment_text, strategies)
        elif task_type == "math_reasoning":
            return self._build_math_reasoning_prompt(comment_text, strategies)
        else:
            # é€šç”¨æ¨¡æ¿
            base_prompt = f"è¯·å¤„ç†ä»¥ä¸‹å†…å®¹ï¼š{task_type}\n\n"
            base_prompt += f"é—®é¢˜ï¼š{comment_text}\nå›ç­”ï¼š"
            return base_prompt

    def _build_text_classification_prompt(self, comment_text: str, strategies: List[str]) -> str:
        """æ„å»ºæ–‡æœ¬åˆ†ç±»æç¤ºè¯ - ç²¾ç¡®åŒ¹é…å›¾ç‰‡éœ€æ±‚"""

        prompt = "ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼š\n\n"
        prompt += f"è¯„è®ºå†…å®¹ï¼š'{comment_text}'\n\n"

        # é›¶æ ·æœ¬æç¤º
        if "zero_shot" in strategies:
            prompt += "åˆ†ç±»æ ‡å‡†è¯´æ˜ï¼š\n"
            prompt += "- æ­£é¢è¯„ä»·ï¼šåŒ…å«èµæ‰¬ã€æ¨èã€å–œçˆ±ç­‰ç§¯ææƒ…æ„Ÿ\n"
            prompt += "- è´Ÿé¢è¯„ä»·ï¼šåŒ…å«æ‰¹è¯„ã€ä¸æ¨èã€ä¸æ»¡ç­‰æ¶ˆææƒ…æ„Ÿ\n"
            prompt += "- ä¸­æ€§è¯„ä»·ï¼šæƒ…æ„Ÿå€¾å‘ä¸æ˜æ˜¾æˆ–æ··åˆæƒ…æ„Ÿ\n\n"

        # å°‘æ ·æœ¬æç¤º
        if "few_shot" in strategies:
            prompt += "å‚è€ƒç¤ºä¾‹ï¼š\n"
            prompt += "è¾“å…¥ï¼š'è¿™éƒ¨ç”µå½±å¤ªç²¾å½©äº†ï¼Œæ¼”å‘˜æ¼”æŠ€å¾ˆæ£’ï¼'\nè¾“å‡ºï¼šæ­£é¢\n"
            prompt += "è¾“å…¥ï¼š'å‰§æƒ…æ— èŠï¼Œç‰¹æ•ˆä¹Ÿå¾ˆå·®ã€‚'\nè¾“å‡ºï¼šè´Ÿé¢\n"
            prompt += "è¾“å…¥ï¼š'æ•´ä½“è¿˜ä¸é”™ï¼Œä½†æœ‰äº›åœ°æ–¹å¯ä»¥æ”¹è¿›ã€‚'\nè¾“å‡ºï¼šä¸­æ€§\n\n"

        # é›¶æ ·æœ¬æ€ç»´é“¾
        if "zero_shot_chain_of_thought" in strategies:
            prompt += "æ€ç»´é“¾åˆ†æï¼š\n"
            prompt += "1. è¯†åˆ«è¯„è®ºä¸­çš„å…³é”®è¯å’Œæƒ…æ„Ÿè¡¨è¾¾\n"
            prompt += "2. åˆ†ææ•´ä½“æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰\n"
            prompt += "3. è€ƒè™‘ä¸Šä¸‹æ–‡å’Œéšå«æƒ…æ„Ÿ\n"
            prompt += "4. ç»™å‡ºæœ€ç»ˆåˆ†ç±»ç»“æœ\n\n"

        prompt += "è¯·è¾“å‡ºåˆ†ç±»ç»“æœï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰ï¼š"
        return prompt

    def _build_information_extraction_prompt(self, text: str, strategies: List[str]) -> str:
        """æ„å»ºä¿¡æ¯æŠ½å–æç¤ºè¯"""
        prompt = "ä¿¡æ¯æŠ½å–ä»»åŠ¡ï¼š\n\n"

        if "zero_shot" in strategies:
            prompt += "æŠ½å–è¯´æ˜ï¼š\n"
            prompt += "- äººåï¼šæå–æ–‡æœ¬ä¸­å‡ºç°çš„**æ‰€æœ‰**äººåï¼ˆæ³¨æ„ï¼šå¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªäººåï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼Œç”¨é€—å·åˆ†éš”ï¼‰\n"
            prompt += "- åœ°ç‚¹ï¼šæå–æ–‡æœ¬ä¸­å‡ºç°çš„**æ‰€æœ‰**åœ°ç‚¹ï¼ˆæ³¨æ„ï¼šå¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªåœ°ç‚¹ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼Œç”¨é€—å·åˆ†éš”ï¼‰\n"
            prompt += "- æ—¶é—´ï¼šæå–æ–‡æœ¬ä¸­å‡ºç°çš„**æ‰€æœ‰**æ—¶é—´ä¿¡æ¯ï¼ˆæ³¨æ„ï¼šå¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªæ—¶é—´ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼Œç”¨é€—å·åˆ†éš”ï¼‰\n\n"

        if "few_shot" in strategies:
            prompt += "å‚è€ƒç¤ºä¾‹ï¼ˆä»…ç”¨äºç†è§£æ ¼å¼ï¼Œä¸è¦æå–è¿™äº›ç¤ºä¾‹ä¸­çš„ä¿¡æ¯ï¼‰ï¼š\n"
            prompt += "ç¤ºä¾‹1 - è¾“å…¥ï¼š'å¼ ä¸‰å’Œç‹äº”å°†äºæ˜å¤©åœ¨åŒ—äº¬å‚åŠ ä¼šè®®'\nç¤ºä¾‹1 - è¾“å‡ºï¼šäººåï¼šå¼ ä¸‰,ç‹äº”ï¼Œåœ°ç‚¹ï¼šåŒ—äº¬ï¼Œæ—¶é—´ï¼šæ˜å¤©\n\n"
            prompt += "ç¤ºä¾‹2 - è¾“å…¥ï¼š'æå››å’Œé©¬å…­æ˜¨å¤©åœ¨ä¸Šæµ·å’Œæ­å·ä¹°äº†æ–°æ‰‹æœº'\nç¤ºä¾‹2 - è¾“å‡ºï¼šäººåï¼šæå››,é©¬å…­ï¼Œåœ°ç‚¹ï¼šä¸Šæµ·,æ­å·ï¼Œæ—¶é—´ï¼šæ˜¨å¤©\n\n"
            prompt += "æ³¨æ„ï¼šå¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªäººåï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼›å¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªåœ°ç‚¹ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼›å¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªæ—¶é—´ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ã€‚\n\n"
            prompt += "ç°åœ¨è¯·å¤„ç†ä»¥ä¸‹å®é™…è¾“å…¥æ–‡æœ¬ï¼ˆæ³¨æ„ï¼šåªä»ä¸‹é¢çš„æ–‡æœ¬å†…å®¹ä¸­æå–ä¿¡æ¯ï¼Œä¸è¦ä»ä¸Šé¢çš„ç¤ºä¾‹ä¸­æå–ï¼‰ï¼š\n\n"

        if "zero_shot_chain_of_thought" in strategies:
            prompt += "æŠ½å–æ­¥éª¤ï¼š\n"
            prompt += "1. ä»”ç»†é˜…è¯»æ–‡æœ¬ï¼Œè¯†åˆ«æ–‡æœ¬ä¸­çš„**æ‰€æœ‰**å®ä½“ï¼ˆäººå/åœ°ç‚¹/æ—¶é—´ï¼‰\n"
            prompt += "2. å¯¹äºæ¯ä¸ªç±»å‹ï¼Œæå–**æ‰€æœ‰**å‡ºç°çš„å®ä½“ï¼ˆä¸è¦é—æ¼ä»»ä½•å®ä½“ï¼‰\n"
            prompt += "3. å¦‚æœåŒä¸€ç±»å‹æœ‰å¤šä¸ªå®ä½“ï¼Œç”¨é€—å·åˆ†éš”\n"
            prompt += "4. æŒ‰æ ¼å¼ç»„ç»‡è¾“å‡º\n\n"

        prompt += f"æ–‡æœ¬å†…å®¹ï¼š'{text}'\n\n"
        prompt += "è¯·ä»”ç»†é˜…è¯»ä¸Šè¿°æ–‡æœ¬å†…å®¹ï¼Œæå–**æ‰€æœ‰**å‡ºç°çš„äººåã€åœ°ç‚¹å’Œæ—¶é—´ä¿¡æ¯ã€‚"
        prompt += "é‡è¦æç¤ºï¼šå¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªäººåï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼›å¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªåœ°ç‚¹ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼›å¦‚æœæ–‡æœ¬ä¸­æœ‰å¤šä¸ªæ—¶é—´ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ã€‚"
        prompt += "æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä»…è¾“å‡ºä¸€æ¡ç»“æœï¼‰ï¼šäººåï¼š<name1,name2,...>ï¼Œåœ°ç‚¹ï¼š<location1,location2,...>ï¼Œæ—¶é—´ï¼š<time1,time2,...>"
        return prompt

    def _build_question_answering_prompt(self, question: str, strategies: List[str]) -> str:
        """æ„å»ºé—®ç­”ä»»åŠ¡æç¤ºè¯"""
        # è§£æè¾“å…¥ï¼Œåˆ†ç¦»æ–‡æœ¬å’Œé—®é¢˜
        text_content = ""
        actual_question = question
        
        # å°è¯•æå–æ–‡æœ¬å’Œé—®é¢˜ - æ”¯æŒå¤šç§æ ¼å¼
        # æ ¼å¼1: æ–‡æœ¬:"..."é—®é¢˜:...
        match = re.search(r'æ–‡æœ¬[:ï¼š]\s*["""]([^"""]+)["""]\s*é—®é¢˜[:ï¼š](.+)', question, re.DOTALL)
        if match:
            text_content = match.group(1).strip()
            actual_question = match.group(2).strip()
        else:
            # æ ¼å¼2: æ–‡æœ¬:'...'é—®é¢˜:...
            match = re.search(r"æ–‡æœ¬[:ï¼š]\s*['']([^'']+)['']\s*é—®é¢˜[:ï¼š](.+)", question, re.DOTALL)
            if match:
                text_content = match.group(1).strip()
                actual_question = match.group(2).strip()
            else:
                # æ ¼å¼3: æ–‡æœ¬:...é—®é¢˜:... (æ²¡æœ‰å¼•å·ï¼Œæ”¯æŒæ¢è¡Œ)
                match = re.search(r'æ–‡æœ¬[:ï¼š]\s*([^é—®é¢˜]+?)\s*é—®é¢˜[:ï¼š](.+)', question, re.DOTALL)
                if match:
                    text_content = match.group(1).strip()
                    actual_question = match.group(2).strip()
                else:
                    # æ ¼å¼4: åªæœ‰é—®é¢˜ï¼Œæ²¡æœ‰æ˜ç¡®çš„æ–‡æœ¬æ ‡è®°
                    # å°è¯•æå–å¼•å·ä¸­çš„å†…å®¹ä½œä¸ºæ–‡æœ¬
                    match = re.search(r"['""]([^'""]+)['""]", question)
                    if match:
                        text_content = match.group(1).strip()
                        # é—®é¢˜éƒ¨åˆ†å¯èƒ½æ˜¯å¼•å·åé¢çš„å†…å®¹
                        match2 = re.search(r"['""][^'""]+['""]\s*é—®é¢˜[:ï¼š](.+)", question)
                        if match2:
                            actual_question = match2.group(1).strip()
        
        prompt = "é—®ç­”ä»»åŠ¡ï¼š\n\n"

        if "zero_shot" in strategies:
            prompt += "å›ç­”è¦æ±‚ï¼š\n"
            prompt += "- åŸºäºæä¾›çš„æ–‡æœ¬ä¿¡æ¯å›ç­”é—®é¢˜\n"
            prompt += "- **é‡è¦ï¼šåªè¾“å‡ºç­”æ¡ˆæœ¬èº«ï¼Œä¸è¦å¤è¿°æ•´ä¸ªæ–‡æœ¬å†…å®¹**\n"
            prompt += "- å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰ç›´æ¥ç­”æ¡ˆï¼Œè¿›è¡Œåˆç†æ¨ç†\n"
            prompt += "- ç­”æ¡ˆè¦ç®€æ´å‡†ç¡®ï¼Œé€šå¸¸åªæœ‰å‡ ä¸ªå­—æˆ–ä¸€ä¸ªçŸ­è¯­\n"
            prompt += "- ä¾‹å¦‚ï¼šå¦‚æœé—®é¢˜æ˜¯å¹´ä»½ï¼Œåªè¾“å‡ºå¹´ä»½ï¼›å¦‚æœæ˜¯åœ°ç‚¹ï¼Œåªè¾“å‡ºåœ°ç‚¹åç§°\n\n"

        if "few_shot" in strategies:
            prompt += "å‚è€ƒç¤ºä¾‹ï¼ˆä»…ç”¨äºç†è§£æ ¼å¼ï¼Œä¸è¦ä½¿ç”¨è¿™äº›ç¤ºä¾‹ä¸­çš„ç­”æ¡ˆï¼‰ï¼š\n"
            prompt += "ç¤ºä¾‹1 - æ–‡æœ¬ï¼š'è‹¹æœå…¬å¸äº1976å¹´åˆ›ç«‹ã€‚' é—®é¢˜ï¼šè‹¹æœå…¬å¸æ˜¯å“ªä¸€å¹´åˆ›ç«‹çš„ï¼Ÿ\nç¤ºä¾‹1 - ç­”æ¡ˆï¼š1976å¹´\n\n"
            prompt += "ç¤ºä¾‹2 - æ–‡æœ¬ï¼š'åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ã€‚' é—®é¢˜ï¼šä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ\nç¤ºä¾‹2 - ç­”æ¡ˆï¼šåŒ—äº¬\n\n"
            prompt += "ç°åœ¨è¯·å¤„ç†ä»¥ä¸‹å®é™…è¾“å…¥ï¼ˆæ³¨æ„ï¼šåªåŸºäºä¸‹é¢çš„æ–‡æœ¬å’Œé—®é¢˜æ¥å›ç­”ï¼Œä¸è¦ä½¿ç”¨ä¸Šé¢ç¤ºä¾‹ä¸­çš„ç­”æ¡ˆï¼‰ï¼š\n\n"

        if "zero_shot_chain_of_thought" in strategies:
            prompt += "å›ç­”æ­¥éª¤ï¼š\n"
            prompt += "1. ç†è§£é—®é¢˜çš„æ ¸å¿ƒå†…å®¹\n"
            prompt += "2. åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯\n"
            prompt += "3. å¦‚éœ€è¦ï¼Œè¿›è¡Œé€»è¾‘æ¨ç†\n"
            prompt += "4. ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ\n\n"

        # å¦‚æœæå–åˆ°äº†æ–‡æœ¬å†…å®¹ï¼Œåˆ™åˆ†åˆ«æ˜¾ç¤º
        if text_content:
            prompt += f"æ–‡æœ¬ï¼š'{text_content}'\n\n"
            prompt += f"é—®é¢˜ï¼š{actual_question}\n\n"
        else:
            # å¦‚æœæ²¡æœ‰æå–åˆ°æ–‡æœ¬ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªè¾“å…¥
            prompt += f"è¾“å…¥ï¼š{question}\n\n"

        prompt += "è¯·åªåŸºäºä¸Šè¿°æ–‡æœ¬å†…å®¹å›ç­”é—®é¢˜ã€‚\n"
        prompt += "**é‡è¦æç¤ºï¼šåªè¾“å‡ºç­”æ¡ˆæœ¬èº«ï¼ˆå¦‚å¹´ä»½ã€åœ°ç‚¹ã€äººåç­‰ï¼‰ï¼Œä¸è¦å¤è¿°æ•´ä¸ªæ–‡æœ¬å†…å®¹ã€‚**\n"
        prompt += "ç­”æ¡ˆï¼š"
        return prompt

    def _build_math_reasoning_prompt(self, question: str, strategies: List[str]) -> str:
        """æ„å»ºæ•°å­¦æ¨ç†æç¤ºè¯"""
        prompt = "æ•°å­¦æ¨ç†ä»»åŠ¡ï¼š\n\n"
        prompt += f"é—®é¢˜ï¼š{question}\n\n"

        if "zero_shot" in strategies:
            prompt += "è§£é¢˜è¦æ±‚ï¼š\n"
            prompt += "- ä»”ç»†åˆ†ææ•°å­¦é—®é¢˜\n"
            prompt += "- æŒ‰æ­¥éª¤è¿›è¡Œè®¡ç®—\n"
            prompt += "- åªè¾“å‡ºæœ€ç»ˆæ•°å­—ç­”æ¡ˆ\n\n"

        if "few_shot" in strategies:
            prompt += "å‚è€ƒç¤ºä¾‹ï¼š\n"
            prompt += "è¾“å…¥ï¼š'2+2=?'\nè¾“å‡ºï¼š4\n"
            prompt += "è¾“å…¥ï¼š'3*5=?'\nè¾“å‡ºï¼š15\n\n"

        if "zero_shot_chain_of_thought" in strategies:
            prompt += "è§£é¢˜æ­¥éª¤ï¼š\n"
            prompt += "1. è¯†åˆ«æ•°å­¦è¿ç®—ç±»å‹\n"
            prompt += "2. æå–æ•°å­—å’Œè¿ç®—ç¬¦\n"
            prompt += "3. æ‰§è¡Œè®¡ç®—\n"
            prompt += "4. éªŒè¯ç»“æœ\n\n"

        prompt += "ç­”æ¡ˆï¼š"
        return prompt

    def _generate_exact_task_analysis(self, task_type: str, comment_text: str) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œå…¨åŒ¹é…å›¾ç‰‡æ ¼å¼çš„ä»»åŠ¡åˆ†æ - æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆå·®å¼‚åŒ–åˆ†æ"""

        analysis = {
            'task_type': task_type,
            'text_length': len(comment_text),
            'complexity_level': self._assess_complexity_level(comment_text),
        }

        # æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆä¸åŒçš„åˆ†æå†…å®¹
        if task_type in ["text_classification", "sentiment_analysis", "sentiment_classification"]:
            analysis['sentiment_indicators'] = {
                'positive': self._find_positive_indicators(comment_text),
                'negative': self._find_negative_indicators(comment_text),
                'is_clear': self._check_sentiment_clarity(comment_text)
            }
            analysis['key_elements'] = self._extract_key_elements(comment_text)
        elif task_type == "information_extraction":
            analysis['entities'] = self._extract_entities_from_text(comment_text)
            analysis['extraction_difficulty'] = self._assess_extraction_difficulty(comment_text)
        elif task_type == "question_answering":
            analysis['question_type'] = self._classify_question_type(comment_text)
            analysis['answerability'] = self._assess_answerability(comment_text)
        elif task_type == "math_reasoning":
            analysis['operation_type'] = self._identify_math_operation(comment_text)
            analysis['complexity_level'] = self._assess_math_complexity(comment_text)
        else:
            # é€šç”¨åˆ†æ
            analysis['key_elements'] = self._extract_key_elements(comment_text)

        return analysis

    def _find_positive_indicators(self, text: str) -> List[str]:
        """æŸ¥æ‰¾æ­£é¢æƒ…æ„ŸæŒ‡ç¤ºè¯"""
        positive_words = ['ç²¾å½©', 'å‡ºè‰²', 'æ¨è', 'ä¼˜ç§€', 'å¾ˆå¥½', 'å¾ˆæ£’', 'å¼ºçƒˆæ¨è', 'éå¸¸ç²¾å½©', 'è¡¨æ¼”å‡ºè‰²']
        return [word for word in positive_words if word in text]

    def _find_negative_indicators(self, text: str) -> List[str]:
        """æŸ¥æ‰¾è´Ÿé¢æƒ…æ„ŸæŒ‡ç¤ºè¯"""
        negative_words = ['ç³Ÿç³•', 'å·®åŠ²', 'ä¸æ¨è', 'æ— èŠ', 'éš¾çœ‹', 'æ‹–æ²“', 'ä¸å¥½']
        return [word for word in negative_words if word in text]

    def _check_sentiment_clarity(self, text: str) -> bool:
        """æ£€æŸ¥æƒ…æ„Ÿæ˜ç¡®æ€§"""
        positive = self._find_positive_indicators(text)
        negative = self._find_negative_indicators(text)
        return len(positive) > 0 or len(negative) > 0

    def _assess_complexity_level(self, text: str) -> str:
        """è¯„ä¼°å¤æ‚åº¦ç­‰çº§"""
        if len(text) < 30:
            return "ä½"
        elif len(text) < 100:
            return "ä¸­"
        else:
            return "é«˜"

    def _extract_key_elements(self, text: str) -> List[str]:
        """æå–å…³é”®å…ƒç´ """
        elements = []
        if "å‰§æƒ…" in text:
            elements.append("å‰§æƒ…è¯„ä»·")
        if "æ¼”å‘˜" in text or "è¡¨æ¼”" in text:
            elements.append("æ¼”å‘˜è¡¨æ¼”")
        if "æ¨è" in text:
            elements.append("æ¨èç¨‹åº¦")
        return elements

    def _extract_entities_from_text(self, text: str) -> Dict[str, List[str]]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“ï¼ˆäººåã€åœ°ç‚¹ã€æ—¶é—´ï¼‰"""
        entities = {'person': [], 'location': [], 'time': []}
        # ç®€å•çš„äººåè¯†åˆ«ï¼ˆä¸­æ–‡å¸¸è§å§“æ°ï¼‰
        person_patterns = ['å¼ ', 'æ', 'ç‹', 'åˆ˜', 'é™ˆ', 'æ¨', 'èµµ', 'é»„', 'å‘¨', 'å´']
        for pattern in person_patterns:
            if pattern in text:
                # æå–å¯èƒ½çš„å§“åï¼ˆ2-3ä¸ªå­—ç¬¦ï¼‰
                matches = re.findall(rf'{pattern}[^ï¼Œã€‚\s]{{0,2}}', text)
                entities['person'].extend(matches[:3])
        
        # åœ°ç‚¹è¯†åˆ«
        location_keywords = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'å—äº¬', 'æ­¦æ±‰', 'æˆéƒ½', 'é‡åº†', 'è¥¿å®‰']
        for loc in location_keywords:
            if loc in text:
                entities['location'].append(loc)
        
        # æ—¶é—´è¯†åˆ«
        time_patterns = ['ä»Šå¤©', 'æ˜å¤©', 'åå¤©', 'æ˜¨å¤©', 'ä¸Šå‘¨', 'ä¸‹å‘¨', 'ä»Šå¹´', 'å»å¹´', 'æ˜å¹´']
        for pattern in time_patterns:
            if pattern in text:
                entities['time'].append(pattern)
        
        return entities

    def _assess_extraction_difficulty(self, text: str) -> str:
        """è¯„ä¼°ä¿¡æ¯æŠ½å–éš¾åº¦"""
        entities = self._extract_entities_from_text(text)
        total_entities = sum(len(v) for v in entities.values())
        if total_entities >= 3:
            return "ä½"
        elif total_entities >= 1:
            return "ä¸­"
        else:
            return "é«˜"

    def _classify_question_type(self, question: str) -> str:
        """åˆ†ç±»é—®é¢˜ç±»å‹"""
        if any(word in question for word in ['ä»€ä¹ˆ', 'å“ª', 'è°', 'å“ªä¸ª']):
            return "äº‹å®æ€§é—®é¢˜"
        elif any(word in question for word in ['ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ']):
            return "è§£é‡Šæ€§é—®é¢˜"
        elif any(word in question for word in ['å¤šå°‘', 'å‡ ä¸ª', 'å‡ ']):
            return "æ•°é‡æ€§é—®é¢˜"
        else:
            return "ä¸€èˆ¬æ€§é—®é¢˜"

    def _assess_answerability(self, question: str) -> str:
        """è¯„ä¼°é—®é¢˜çš„å¯å›ç­”æ€§"""
        if len(question) < 20:
            return "å¯èƒ½ç¼ºå°‘ä¸Šä¸‹æ–‡"
        elif any(word in question for word in ['æ ¹æ®', 'åŸºäº', 'æ–‡æœ¬']):
            return "éœ€è¦ä¸Šä¸‹æ–‡ä¿¡æ¯"
        else:
            return "å¯ç›´æ¥å›ç­”"

    def _identify_math_operation(self, text: str) -> str:
        """è¯†åˆ«æ•°å­¦è¿ç®—ç±»å‹"""
        if any(op in text for op in ['+', 'åŠ ', 'åŠ ä¸Š']):
            return "åŠ æ³•"
        elif any(op in text for op in ['-', 'å‡', 'å‡å»']):
            return "å‡æ³•"
        elif any(op in text for op in ['*', 'Ã—', 'ä¹˜', 'ä¹˜ä»¥']):
            return "ä¹˜æ³•"
        elif any(op in text for op in ['/', 'Ã·', 'é™¤', 'é™¤ä»¥']):
            return "é™¤æ³•"
        else:
            return "æœªçŸ¥è¿ç®—"

    def _assess_math_complexity(self, text: str) -> str:
        """è¯„ä¼°æ•°å­¦é—®é¢˜å¤æ‚åº¦"""
        numbers = re.findall(r'\d+', text)
        if len(numbers) <= 2:
            return "ä½"
        elif len(numbers) <= 4:
            return "ä¸­"
        else:
            return "é«˜"

    def _generate_complexity_assessment(self, text: str) -> str:
        """ç”Ÿæˆå¤æ‚åº¦è¯„ä¼° - ä¿®å¤N/Aé—®é¢˜"""
        # ç®€å•çš„å¤æ‚åº¦è¯„ä¼°é€»è¾‘
        if len(text) < 20:
            return "ä½ (æ–‡æœ¬ç®€çŸ­)"
        elif len(text) < 50:
            return "ä¸­ (ä¸­ç­‰é•¿åº¦)"
        else:
            return "é«˜ (æ–‡æœ¬è¾ƒé•¿)"

    def _heuristic_quality_and_suggestions(self, task_type: str, text: str, strategies: List[str]) -> Tuple[float, List[str]]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹ã€é•¿åº¦ã€æ¸…æ™°åº¦ã€ç­–ç•¥ç­‰ç»™å‡ºè´¨é‡åˆ†ä¸æ”¹è¿›å»ºè®®ã€‚"""
        score = 0.5
        suggestions = []

        length = len(text)
        if length >= 20:
            score += 0.15
        else:
            suggestions.append("è¡¥å……æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæå‡åˆ¤åˆ«ä¾æ®")

        # æ ¹æ®ä»»åŠ¡ç±»å‹ç»™å‡ºé’ˆå¯¹æ€§è¯„åˆ†å’Œå»ºè®®
        if task_type in ["text_classification", "sentiment_analysis", "sentiment_classification"]:
            if self._check_sentiment_clarity(text):
                score += 0.2
            else:
                suggestions.append("æ·»åŠ æ˜¾å¼çš„æ­£/è´Ÿé¢çº¿ç´¢ï¼Œæˆ–æä¾›ä¸­æ€§è¯­å¢ƒè¯´æ˜")
        elif task_type == "information_extraction":
            entities = self._extract_entities_from_text(text)
            total_entities = sum(len(v) for v in entities.values())
            if total_entities >= 2:
                score += 0.2
            else:
                suggestions.append("ç¡®ä¿æ–‡æœ¬ä¸­åŒ…å«äººåã€åœ°ç‚¹æˆ–æ—¶é—´ç­‰å¯æŠ½å–å®ä½“")
        elif task_type == "question_answering":
            if any(word in text for word in ['æ ¹æ®', 'åŸºäº', 'æ–‡æœ¬', 'é—®é¢˜']):
                score += 0.2
            else:
                suggestions.append("ç¡®ä¿é—®é¢˜æ˜ç¡®ï¼Œå¹¶åŒ…å«å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯")
        elif task_type == "math_reasoning":
            numbers = re.findall(r'\d+', text)
            if len(numbers) >= 2:
                score += 0.2
            else:
                suggestions.append("ç¡®ä¿æ•°å­¦é—®é¢˜åŒ…å«è¶³å¤Ÿçš„æ•°å­—å’Œè¿ç®—ç¬¦å·")

        if strategies and ("few_shot" in strategies):
            score += 0.15
        else:
            if task_type == "information_extraction":
                suggestions.append("ä¿¡æ¯æŠ½å–ä»»åŠ¡å»ºè®®ä½¿ç”¨few_shotç­–ç•¥ï¼Œæä¾›ç¤ºä¾‹æ ¼å¼")
            elif task_type == "question_answering":
                suggestions.append("é—®ç­”ä»»åŠ¡å»ºè®®ä½¿ç”¨few_shotç­–ç•¥ï¼Œå±•ç¤ºé—®ç­”æ ¼å¼")
            else:
                suggestions.append("è€ƒè™‘åŠ å…¥å°‘æ ·æœ¬ç¤ºä¾‹ä»¥ç¨³å®šè¾“å‡º")

        return min(1.0, round(score, 2)), suggestions


class DSPyEvaluationMetric(ABC):
    """DSPyè¯„ä¼°æŒ‡æ ‡çš„æŠ½è±¡åŸºç±»"""

    @abstractmethod
    def evaluate(self, example: dspy.Example, pred: dspy.Prediction) -> bool:
        pass


class DSPySentimentMetric(DSPyEvaluationMetric):
    """DSPyæƒ…æ„Ÿåˆ†æè¯„ä¼°æŒ‡æ ‡"""

    def evaluate(self, example: dspy.Example, pred: dspy.Prediction) -> bool:
        # è·å–é¢„æµ‹è¾“å‡º
        pred_output = ""
        if hasattr(pred, 'sentiment'):
            pred_output = str(pred.sentiment)
        elif hasattr(pred, 'output'):
            pred_output = str(pred.output)
        else:
            # å°è¯•ä»predçš„æ‰€æœ‰å±æ€§ä¸­æŸ¥æ‰¾
            for attr in ['sentiment', 'output', 'answer', 'label']:
                if hasattr(pred, attr):
                    pred_output = str(getattr(pred, attr))
                    break

        # è·å–çœŸå®è¾“å‡º
        true_output = ""
        if hasattr(example, 'sentiment'):
            true_output = str(example.sentiment)
        elif hasattr(example, 'output'):
            true_output = str(example.output)
        else:
            # å°è¯•ä»exampleçš„æ‰€æœ‰å±æ€§ä¸­æŸ¥æ‰¾
            for attr in ['sentiment', 'output', 'answer', 'label']:
                if hasattr(example, attr):
                    true_output = str(getattr(example, attr))
                    break

        pred_output = pred_output.strip().lower()
        true_output = true_output.strip().lower()

        print(f"  ğŸ” æƒ…æ„Ÿè¯„ä¼°: é¢„æµ‹='{pred_output}', æœŸæœ›='{true_output}'")

        # ç›´æ¥åŒ¹é…
        if pred_output == true_output:
            return True

        # æå–æ•°å­—æ ‡ç­¾è¿›è¡ŒåŒ¹é…
        pred_num = re.search(r'[01]', pred_output)
        true_num = re.search(r'[01]', true_output)
        if pred_num and true_num:
            return pred_num.group() == true_num.group()

        # å…³é”®è¯åŒ¹é…ï¼ˆä½œä¸ºåå¤‡æ–¹æ¡ˆï¼‰
        pred_positive = any(word in pred_output for word in ['æ­£é¢', 'ç§¯æ', 'positive', '1', 'å¥½'])
        pred_negative = any(word in pred_output for word in ['è´Ÿé¢', 'æ¶ˆæ', 'negative', '0', 'ä¸å¥½'])
        true_positive = any(word in true_output for word in ['æ­£é¢', 'ç§¯æ', 'positive', '1', 'å¥½'])
        true_negative = any(word in true_output for word in ['è´Ÿé¢', 'æ¶ˆæ', 'negative', '0', 'ä¸å¥½'])
        
        if (pred_positive and true_positive) or (pred_negative and true_negative):
            return True
        if (pred_positive and true_negative) or (pred_negative and true_positive):
            return False

        return False


class DSPyMathMetric(DSPyEvaluationMetric):
    """DSPyæ•°å­¦æ¨ç†è¯„ä¼°æŒ‡æ ‡"""

    def evaluate(self, example: dspy.Example, pred: dspy.Prediction) -> bool:
        # è·å–é¢„æµ‹è¾“å‡º
        pred_output = ""
        if hasattr(pred, 'answer'):
            pred_output = str(pred.answer)
        elif hasattr(pred, 'output'):
            pred_output = str(pred.output)

        # è·å–çœŸå®è¾“å‡º
        true_output = ""
        if hasattr(example, 'answer'):
            true_output = str(example.answer)
        elif hasattr(example, 'output'):
            true_output = str(example.output)

        pred_output = pred_output.strip()
        true_output = true_output.strip()

        print(f"  ğŸ” æ•°å­¦è¯„ä¼°: é¢„æµ‹='{pred_output}', æœŸæœ›='{true_output}'")

        if pred_output == true_output:
            return True

        pred_nums = re.findall(r'[-+]?\d*\.\d+|\d+', pred_output)
        true_nums = re.findall(r'[-+]?\d*\.\d+|\d+', true_output)

        if pred_nums and true_nums:
            return pred_nums[0] == true_nums[0]

        return False


class DSPyTaskEvaluator:
    """DSPyç»Ÿä¸€å¤šä»»åŠ¡è¯„æµ‹å™¨"""

    def __init__(self, cost_tracker):
        self.cost_tracker = cost_tracker
        self.metric_strategies = {
            "sentiment_classification": DSPySentimentMetric(),
            "math_reasoning": DSPyMathMetric(),
            "sentiment_analysis": DSPySentimentMetric(),
            "text_classification": DSPySentimentMetric(),
            "default": DSPyMathMetric()
        }

    def evaluate_task(self, predictor, test_data: List[dspy.Example],
                      task_name: str) -> Dict[str, float]:
        """è¯„æµ‹å•ä¸ªä»»åŠ¡"""
        if not test_data:
            return {"accuracy": 0.0, "latency": 0.0, "cost": 0.0, "total_tokens": 0, "samples_tested": 0}

        metric_strategy = self.metric_strategies.get(task_name, self.metric_strategies["default"])

        start_time = time.time()
        accuracy, detailed_results = self._calculate_accuracy(predictor, test_data, metric_strategy)
        latency = time.time() - start_time

        return {
            "accuracy": round(accuracy, 3),
            "latency": round(latency, 2),
            "cost": self.cost_tracker.get_cost(),
            "total_tokens": self.cost_tracker.total_tokens,
            "samples_tested": len(test_data),
            "detailed_results": detailed_results
        }

    def _calculate_accuracy(self, predictor, test_data, metric_strategy):
        """è®¡ç®—å‡†ç¡®ç‡"""
        correct = 0
        detailed_results = []

        for i, example in enumerate(test_data):
            try:
                # è·å–è¾“å…¥å­—æ®µ
                input_fields = {}
                for k, v in example.items():
                    if k != 'output' and k in getattr(example, '_input_fields', example.keys()):
                        input_fields[k] = v

                print(f"  ğŸ“‹ è¾“å…¥å­—æ®µ: {input_fields}")

                # æ­£ç¡®è°ƒç”¨é¢„æµ‹å™¨
                pred = predictor(**input_fields)

                # è·å–é¢„æµ‹è¾“å‡ºå’ŒçœŸå®è¾“å‡º
                pred_output = self._get_prediction_output(pred)
                true_output = self._get_example_output(example)

                print(f"  ğŸ“Š é¢„æµ‹è¾“å‡º: {pred_output}")
                print(f"  ğŸ“Š æœŸæœ›è¾“å‡º: {true_output}")

                # è¯„ä¼°
                is_correct = metric_strategy.evaluate(example, pred)
                if is_correct:
                    correct += 1

                detailed_results.append({
                    'sample': i + 1,
                    'predicted': str(pred_output),
                    'expected': str(true_output),
                    'correct': is_correct
                })

                print(f"  ğŸ“ æ ·æœ¬ {i + 1}: é¢„æµ‹='{pred_output}', æœŸæœ›='{true_output}', æ­£ç¡®={is_correct}")

            except Exception as e:
                print(f"  âŒ è¯„ä¼°å‡ºé”™: {str(e)}")
                detailed_results.append({
                    'sample': i + 1,
                    'error': str(e),
                    'correct': False
                })

        accuracy = correct / len(test_data) if test_data else 0.0
        return accuracy, detailed_results

    def _get_prediction_output(self, pred):
        """ä»é¢„æµ‹å¯¹è±¡ä¸­è·å–è¾“å‡º"""
        if hasattr(pred, 'answer'):
            return str(pred.answer)
        elif hasattr(pred, 'sentiment'):
            return str(pred.sentiment)
        elif hasattr(pred, 'output'):
            return str(pred.output)
        else:
            # å°è¯•è·å–æ‰€æœ‰å±æ€§
            for attr in dir(pred):
                if not attr.startswith('_'):
                    value = getattr(pred, attr)
                    if value and str(value) not in ['', 'None']:
                        return str(value)
            return ""

    def _get_example_output(self, example):
        """ä»ç¤ºä¾‹ä¸­è·å–è¾“å‡º"""
        # å°è¯•å¤šç§å­—æ®µå
        for attr in ['answer', 'output', 'sentiment', 'label']:
            if hasattr(example, attr):
                value = getattr(example, attr)
                if value and str(value) not in ['', 'None']:
                    return str(value)
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•ä»å­—å…¸ä¸­è·å–
        if isinstance(example, dict):
            for key in ['answer', 'output', 'sentiment', 'label']:
                if key in example:
                    value = example[key]
                    if value and str(value) not in ['', 'None']:
                        return str(value)
        
        # æœ€åå°è¯•è·å–æ‰€æœ‰éç§æœ‰å±æ€§
        for attr in dir(example):
            if not attr.startswith('_') and attr not in ['with_inputs', 'items', 'keys']:
                try:
                    value = getattr(example, attr)
                    if value and str(value) not in ['', 'None', '<bound method', '<function']:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–¹æ³•æˆ–å‡½æ•°
                        if not callable(value):
                            return str(value)
                except:
                    pass
        
        return ""


class DSPyOllamaClient:
    """DSPy Ollamaå®¢æˆ·ç«¯"""

    def __init__(self, model: str, api_base: str = "http://localhost:11434", **kwargs):
        self.model = model
        self.api_base = api_base
        self.kwargs = kwargs
        self.history = []

    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        try:
            url = f"{self.api_base}/api/generate"
            data = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                **self.kwargs,
                **kwargs
            }

            print(f"  ğŸ¤– å‘é€è¯·æ±‚åˆ°Ollama: {prompt[:100]}...")
            response = requests.post(url, json=data, timeout=60)
            response.raise_for_status()
            result = response.json()

            response_text = result.get('response', '')
            print(f"  ğŸ“¨ Ollamaå“åº”: {response_text}")

            self.history.append({
                'prompt': prompt,
                'response': response_text,
                'tokens': result.get('eval_count', 0)
            })

            return response_text

        except Exception as e:
            print(f"  âŒ Ollamaè¯·æ±‚é”™è¯¯: {e}")
            return f"é”™è¯¯: {e}"


class DSPyBasicPredictor:
    """DSPyåŸºç¡€é¢„æµ‹å™¨"""

    def __init__(self, signature, ollama_client, task_type=""):
        self.signature = signature
        self.ollama = ollama_client
        self.task_type = task_type

    def __call__(self, **kwargs):
        """è°ƒç”¨é¢„æµ‹å™¨ - ä½¿ç”¨å…³é”®å­—å‚æ•°"""
        try:
            print(f"  ğŸ¯ è°ƒç”¨é¢„æµ‹å™¨ï¼Œä»»åŠ¡ç±»å‹: {self.task_type}")
            print(f"  ğŸ“¥ è¾“å…¥å‚æ•°: {kwargs}")

            # æ„å»ºæç¤ºè¯
            if self.task_type == "math_reasoning":
                prompt = self._build_math_prompt(kwargs)
            elif self.task_type in ["sentiment_analysis", "text_classification"]:
                prompt = self._build_sentiment_prompt(kwargs)
            else:
                prompt = self._build_general_prompt(kwargs)

            # è°ƒç”¨Ollama
            response = self.ollama.generate(prompt, max_tokens=50)

            # æ¸…ç†å“åº”
            response = response.strip()

            # æ ¹æ®ä»»åŠ¡ç±»å‹è§£æå“åº”å¹¶è¿”å›æ­£ç¡®çš„Predictionå¯¹è±¡
            if self.task_type == "math_reasoning":
                answer = self._parse_math_response(response)
                print(f"  ğŸ§® æ•°å­¦ç­”æ¡ˆ: {answer}")
                return dspy.Prediction(answer=answer)
            elif self.task_type in ["sentiment_analysis", "text_classification"]:
                sentiment = self._parse_sentiment_response(response)
                print(f"  ğŸ˜Š æƒ…æ„Ÿåˆ†æ: {sentiment}")
                return dspy.Prediction(sentiment=sentiment)
            else:
                print(f"  ğŸ“ é€šç”¨å“åº”: {response}")
                return dspy.Prediction(output=response)

        except Exception as e:
            print(f"  âŒ é¢„æµ‹é”™è¯¯: {e}")
            # è¿”å›é»˜è®¤é¢„æµ‹
            if self.task_type == "math_reasoning":
                return dspy.Prediction(answer="é”™è¯¯")
            elif self.task_type in ["sentiment_analysis", "text_classification"]:
                return dspy.Prediction(sentiment="0")
            else:
                return dspy.Prediction(output="é”™è¯¯")

    def _build_math_prompt(self, kwargs):
        """æ„å»ºæ•°å­¦æç¤ºè¯"""
        question = kwargs.get('question', '')
        prompt = f"""è¯·å›ç­”ä»¥ä¸‹æ•°å­¦é—®é¢˜ï¼Œåªè¾“å‡ºæ•°å­—ç­”æ¡ˆï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚

é—®é¢˜: {question}

ç­”æ¡ˆ:"""
        return prompt

    def _build_sentiment_prompt(self, kwargs):
        """æ„å»ºæƒ…æ„Ÿåˆ†ææç¤ºè¯"""
        text = kwargs.get('text', '')
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿï¼Œå¦‚æœæ˜¯æ­£é¢æƒ…æ„Ÿè¾“å‡º1ï¼Œè´Ÿé¢æƒ…æ„Ÿè¾“å‡º0ã€‚

æ–‡æœ¬: {text}

æƒ…æ„Ÿ:"""
        return prompt

    def _build_general_prompt(self, kwargs):
        """æ„å»ºé€šç”¨æç¤ºè¯"""
        prompt_parts = []
        for key, value in kwargs.items():
            prompt_parts.append(f"{key}: {value}")
        return "\n".join(prompt_parts) + "\nå›ç­”:"

    def _parse_math_response(self, response):
        """è§£ææ•°å­¦å“åº”"""
        # æå–æ•°å­—
        numbers = re.findall(r'\d+', response)
        if numbers:
            return numbers[0]
        return response

    def _parse_sentiment_response(self, response):
        """è§£ææƒ…æ„Ÿå“åº”"""
        # æå–0æˆ–1
        sentiment = re.search(r'[01]', response)
        if sentiment:
            return sentiment.group()
        # åŸºäºå…³é”®è¯åˆ¤æ–­
        if any(word in response.lower() for word in ['æ­£é¢', 'ç§¯æ', 'å¥½', 'å¼€å¿ƒ', 'é«˜å…´', '1']):
            return "1"
        elif any(word in response.lower() for word in ['è´Ÿé¢', 'æ¶ˆæ', 'ä¸å¥½', 'ä¼¤å¿ƒ', 'éš¾è¿‡', '0']):
            return "0"
        return response


class DSPyPipelineOptimizer:
    """DSPyå…¨æµç¨‹ä¼˜åŒ–å™¨"""

    def __init__(self, lm_config: Dict[str, Any]):
        self.ollama_client = self._init_ollama_client(lm_config)
        self.prompt_optimizer = DSPyPromptOptimizer(self.ollama_client)
        self.cost_tracker = DSPyCostTracker()
        self.evaluator = DSPyTaskEvaluator(self.cost_tracker)

    def optimize_prompt(self, task_type: str, input_question: str,
                        strategies: List[str] = None, model_type: str = "local") -> Dict[str, Any]:
        """
        ä¼˜åŒ–æç¤ºè¯ - å®Œå…¨åŒ¹é…å›¾ç‰‡ç•Œé¢
        """
        return self.prompt_optimizer.optimize_prompt(task_type, input_question, strategies, model_type)

    def self_consistent_answer(self, task_type: str, inputs: Dict[str, Any],
                               num_samples: int = 5) -> Dict[str, Any]:
        """ä¸€è‡´æ€§é‡‡æ · + å¤šæ•°æŠ•ç¥¨ã€‚

        å‚æ•°:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¦‚ text_classification, sentiment_analysis, math_reasoning ç­‰ï¼‰
            inputs: ä¸è¯¥ä»»åŠ¡ç­¾ååŒ¹é…çš„è¾“å…¥å­—æ®µï¼Œå¦‚ {"text": "..."} æˆ– {"question": "..."}
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡

        è¿”å›:
            {
              "answer": æœ€ç»ˆæŠ•ç¥¨ç»“æœ,
              "all_samples": [str, ...],
              "vote_detail": {å€™é€‰: ç¥¨æ•°}
            }
        """
        predictor = DSPyBasicPredictor(signature=object(), ollama_client=self.ollama_client, task_type=task_type)

        samples = []
        for _ in range(max(1, num_samples)):
            try:
                pred = predictor(**inputs)
                # å¤ç”¨ evaluator çš„è¾“å‡ºæå–é€»è¾‘
                value = self.evaluator._get_prediction_output(pred)
                samples.append(str(value).strip())
            except Exception as e:
                samples.append(f"é”™è¯¯:{e}")

        # å¤šæ•°æŠ•ç¥¨
        counter = Counter(samples)
        final_answer, _ = counter.most_common(1)[0]
        return {
            "answer": final_answer,
            "all_samples": samples,
            "vote_detail": dict(counter)
        }

    def predict_once(self, task_type: str, inputs: Dict[str, Any]) -> str:
        """æ‰§è¡Œä¸€æ¬¡é¢„æµ‹ï¼Œè¿”å›å­—ç¬¦ä¸²ç»“æœã€‚"""
        predictor = DSPyBasicPredictor(signature=object(), ollama_client=self.ollama_client, task_type=task_type)
        pred = predictor(**inputs)
        return self.evaluator._get_prediction_output(pred)

    def automated_prompt_search(self, task_type: str,
                                sample_questions: List[Dict[str, Any]],
                                candidate_strategies: Optional[List[str]] = None) -> Dict[str, Any]:
        """è‡ªåŠ¨æç¤º/æ¨¡æ¿æœç´¢ï¼ˆç®€ç‰ˆï¼‰ï¼š
        - é’ˆå¯¹ç»™å®šä»»åŠ¡åœ¨å¤šç§æç¤ºç­–ç•¥ä¸Šåšå°è§„æ¨¡è¯„æµ‹
        - é€‰æ‹©æ€§èƒ½æ›´ä¼˜çš„ç­–ç•¥å¹¶ç»™å‡ºä¼˜åŒ–æ¨¡æ¿ä¸ä¼°è®¡æ€§èƒ½

        è¿”å›å­—æ®µæ»¡è¶³å‰ç«¯è°ƒç”¨ï¼špatterns_analysis, strategy_recommendations, best_strategy,
        optimized_template, performance_estimate
        """
        if candidate_strategies is None:
            candidate_strategies = ["zero_shot", "few_shot", "zero_shot_chain_of_thought"]

        # åŸºäºæ ·ä¾‹æˆ–å†…ç½®æ ·ä¾‹æ„é€ æµ‹è¯•é›†
        sample_data = create_sample_data()
        if task_type in ["text_classification", "sentiment_analysis", "sentiment_classification"]:
            # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ä¼ å…¥çš„ç¤ºä¾‹æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å†…ç½®æ•°æ®
            test_examples = []
            if sample_questions:
                for item in sample_questions:
                    q = item.get("question", "")
                    a = item.get("answer", "")
                    # ä»é—®é¢˜ä¸­æå–æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœé—®é¢˜åŒ…å«"è¯„è®ºï¼š'...'"æ ¼å¼ï¼‰
                    import re
                    text_match = re.search(r"è¯„è®º[:ï¼š]['""]([^'""]+)['""]", q)
                    if text_match:
                        text_content = text_match.group(1)
                    else:
                        # å¦‚æœæ²¡æœ‰å¼•å·ï¼Œå°è¯•æå–"è¯„è®ºï¼š"åé¢çš„å†…å®¹
                        text_match = re.search(r"è¯„è®º[:ï¼š]([^ï¼Œã€‚ï¼Ÿ]+)", q)
                        text_content = text_match.group(1).strip() if text_match else q
                    
                    # è§„èŒƒåŒ–ç­”æ¡ˆï¼šå°†"æ­£é¢"ã€"ç§¯æ"ç­‰æ˜ å°„ä¸º"1"ï¼Œ"è´Ÿé¢"ã€"æ¶ˆæ"ç­‰æ˜ å°„ä¸º"0"
                    answer_lower = a.lower().strip()
                    if "æ­£é¢" in answer_lower or "ç§¯æ" in answer_lower or "positive" in answer_lower:
                        sentiment_label = "1"
                    elif "è´Ÿé¢" in answer_lower or "æ¶ˆæ" in answer_lower or "negative" in answer_lower:
                        sentiment_label = "0"
                    elif "ä¸­æ€§" in answer_lower or "neutral" in answer_lower:
                        sentiment_label = "0"  # ä¸­æ€§æ˜ å°„ä¸º0ï¼Œä»¥é€‚é…äºŒåˆ†ç±»
                    else:
                        # å°è¯•æå–æ•°å­—
                        num_match = re.search(r'[01]', answer_lower)
                        sentiment_label = num_match.group() if num_match else a
                    
                    ex = dspy.Example(text=text_content, sentiment=sentiment_label).with_inputs("text")
                    test_examples.append(ex)
            
            # å¦‚æœæ²¡æœ‰ç”¨æˆ·æä¾›çš„ç¤ºä¾‹ï¼Œä½¿ç”¨å†…ç½®æ•°æ®
            if not test_examples:
                test_examples = sample_data["sentiment_test"]
            
            # è°ƒè¯•ä¿¡æ¯ï¼šç¡®ä¿test_examplesä¸ä¸ºç©º
            print(f"  ğŸ“‹ æ–‡æœ¬åˆ†ç±»æµ‹è¯•é›†å¤§å°: {len(test_examples)}")
            if test_examples:
                print(f"  ğŸ“‹ ç¤ºä¾‹1: text='{getattr(test_examples[0], 'text', 'N/A')}', sentiment='{getattr(test_examples[0], 'sentiment', 'N/A')}'")
            else:
                print(f"  âš ï¸ è­¦å‘Š: test_examplesä¸ºç©ºï¼")
        elif task_type in ["math_reasoning"]:
            test_examples = sample_data["math_test"]
        elif task_type in ["question_answering"]:
            # å°†ä¼ å…¥çš„æ ·ä¾‹ç”¨äºé—®ç­”è¯„æµ‹ï¼Œè‹¥æ— æ ·ä¾‹ç»™å‡ºé»˜è®¤
            test_examples = []
            if sample_questions:
                for item in sample_questions:
                    q = item.get("question", "")
                    a = item.get("answer", "")
                    if q and a:
                        ex = dspy.Example(question=q, answer=a).with_inputs("question")
                        test_examples.append(ex)
            if not test_examples:
                # æä¾›é»˜è®¤æ ·ä¾‹
                ex = dspy.Example(question="æ ¹æ®ä»¥ä¸‹æ–‡æœ¬å›ç­”é—®é¢˜ï¼š'è‹¹æœå…¬å¸äº1976å¹´4æœˆ1æ—¥ç”±å²è’‚å¤«Â·ä¹”å¸ƒæ–¯ã€å²è’‚å¤«Â·æ²ƒå…¹å°¼äºšå…‹å’Œç½—çº³å¾·Â·éŸ¦æ©åˆ›ç«‹ã€‚' é—®é¢˜ï¼šè‹¹æœå…¬å¸æ˜¯å“ªä¸€å¹´åˆ›ç«‹çš„ï¼Ÿ", answer="1976å¹´").with_inputs("question")
                test_examples = [ex]
            
            # è°ƒè¯•ä¿¡æ¯ï¼šç¡®ä¿test_examplesä¸ä¸ºç©º
            print(f"  ğŸ“‹ é—®ç­”ä»»åŠ¡æµ‹è¯•é›†å¤§å°: {len(test_examples)}")
            if test_examples:
                print(f"  ğŸ“‹ ç¤ºä¾‹1: question='{getattr(test_examples[0], 'question', 'N/A')[:50]}...', answer='{getattr(test_examples[0], 'answer', 'N/A')}'")
            else:
                print(f"  âš ï¸ è­¦å‘Š: é—®ç­”ä»»åŠ¡test_examplesä¸ºç©ºï¼")
        elif task_type in ["information_extraction"]:
            # å°†ä¼ å…¥çš„ sample_questions è½¬ä¸ºç®€æ˜“ Exampleï¼štext -> question, output -> answer
            # æ³¨æ„ï¼šè¿™é‡Œçš„textå­—æ®µå­˜å‚¨çš„æ˜¯å®Œæ•´çš„é—®é¢˜æ–‡æœ¬ï¼ˆåŒ…å«"æå–..."ç­‰ï¼‰ï¼Œç”¨äºåŠ¨æ€ç”Ÿæˆæ¨¡æ¿
            test_examples = []
            for item in (sample_questions or []):
                q = item.get("question", "")
                a = item.get("answer", "")
                # å­˜å‚¨å®Œæ•´é—®é¢˜æ–‡æœ¬ï¼Œç”¨äºæ¨¡æ¿ç”Ÿæˆ
                ex = dspy.Example(text=q, output=a, question_text=q).with_inputs("text")
                test_examples.append(ex)
            if not test_examples:
                # æä¾›ä¸€ä¸ªé»˜è®¤æ ·ä¾‹
                ex = dspy.Example(text="ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–äººåã€åœ°ç‚¹å’Œæ—¶é—´ï¼š'å¼ ä¸‰å°†äºæ˜å¤©åœ¨åŒ—äº¬å‚åŠ ä¼šè®®'", 
                                 output="äººåï¼šå¼ ä¸‰ï¼Œåœ°ç‚¹ï¼šåŒ—äº¬ï¼Œæ—¶é—´ï¼šæ˜å¤©",
                                 question_text="ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–äººåã€åœ°ç‚¹å’Œæ—¶é—´ï¼š'å¼ ä¸‰å°†äºæ˜å¤©åœ¨åŒ—äº¬å‚åŠ ä¼šè®®'").with_inputs("text")
                test_examples = [ex]
        else:
            # é»˜è®¤å›é€€åˆ°æƒ…æ„Ÿæµ‹è¯•é›†
            test_examples = sample_data["sentiment_test"]

        # æ„é€ â€œç­–ç•¥ -> æ¨¡æ¿â€ï¼Œå¹¶åšå°è§„æ¨¡è¯„æµ‹
        performance = []
        for strategy in candidate_strategies:
            # ç»„è£…ä¸€ä¸ªç¤ºæ„è¾“å…¥
            if task_type in ["text_classification", "sentiment_analysis", "sentiment_classification"]:
                demo_text = sample_questions[0]["question"] if sample_questions else "è¿™æ˜¯ä¸€æ¡ç¤ºä¾‹æ–‡æœ¬"
                optimized_prompt = self.prompt_optimizer._generate_exact_prompt_for_ui(
                    task_type, demo_text, [strategy]
                )
            elif task_type == "math_reasoning":
                demo_text = sample_questions[0]["question"] if sample_questions else "2+2=?"
                optimized_prompt = f"è¯·å›ç­”ä»¥ä¸‹æ•°å­¦é—®é¢˜ï¼Œåªè¾“å‡ºæ•°å­—ç­”æ¡ˆï¼š\n\né—®é¢˜ï¼š{demo_text}\n\nç­”æ¡ˆï¼š"
            elif task_type == "information_extraction":
                demo_text = sample_questions[0]["question"] if sample_questions else "ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–äººåã€åœ°ç‚¹å’Œæ—¶é—´ï¼š'å¼ ä¸‰å°†äºæ˜å¤©åœ¨åŒ—äº¬å‚åŠ ä¼šè®®'"
                # æ ¹æ®é—®é¢˜å†…å®¹åŠ¨æ€ç”Ÿæˆæå–æ¨¡æ¿
                # ä»é—®é¢˜ä¸­æå–å®é™…æ–‡æœ¬å†…å®¹
                import re
                text_match = re.search(r"['""]([^'""]+)['""]", demo_text)
                if text_match:
                    actual_text = text_match.group(1)
                else:
                    # å¦‚æœæ²¡æœ‰å¼•å·ï¼Œå°è¯•æå–"ä»"æˆ–"æå–"åé¢çš„å†…å®¹
                    text_match = re.search(r"(?:ä»|æå–)[^ï¼š:]*[ï¼š:]([^ï¼Œã€‚ï¼Ÿ]+)", demo_text)
                    actual_text = text_match.group(1).strip() if text_match else demo_text
                
                extraction_template = self.prompt_optimizer._generate_extraction_template(demo_text)
                optimized_prompt = extraction_template.format(text=actual_text)
            elif task_type == "question_answering":
                demo_text = sample_questions[0]["question"] if sample_questions else "æ ¹æ®ä»¥ä¸‹æ–‡æœ¬å›ç­”é—®é¢˜ï¼š'è‹¹æœå…¬å¸äº1976å¹´åˆ›ç«‹ã€‚' é—®é¢˜ï¼šè‹¹æœå…¬å¸æ˜¯å“ªä¸€å¹´åˆ›ç«‹çš„ï¼Ÿ"
                # ä½¿ç”¨ä¸“é—¨çš„æç¤ºè¯ç”Ÿæˆæ–¹æ³•ï¼Œç¡®ä¿æ­£ç¡®è§£æé—®é¢˜å’Œæ–‡æœ¬
                optimized_prompt = self.prompt_optimizer._build_question_answering_prompt(demo_text, [strategy])
            else:
                demo_text = sample_questions[0]["question"] if sample_questions else "è¯·å®Œæˆä»»åŠ¡"
                optimized_prompt = f"ä»»åŠ¡ï¼š{task_type}\n\né—®é¢˜ï¼š{demo_text}\nå›ç­”ï¼š"

            # ç”¨è¯¥ç­–ç•¥çš„é¢„æµ‹å™¨è·‘ä¸€éæµ‹è¯•é›†ï¼ˆè½»é‡ï¼‰
            predictor = DSPyBasicPredictor(signature=object(), ollama_client=self.ollama_client, task_type=task_type)

            # åŒ…ä¸€å±‚è°ƒç”¨ä»¥å›ºå®šç­–ç•¥æ¨¡æ¿
            def run_with_template(example: dspy.Example):
                import time
                start_time = time.time()
                
                if task_type in ["text_classification", "sentiment_analysis", "sentiment_classification"]:
                    text = getattr(example, "text", "")
                    prompt = self.prompt_optimizer._generate_exact_prompt_for_ui(task_type, text, [strategy])
                elif task_type == "math_reasoning":
                    q = getattr(example, "question", "")
                    prompt = f"è¯·å›ç­”ä»¥ä¸‹æ•°å­¦é—®é¢˜ï¼Œåªè¾“å‡ºæ•°å­—ç­”æ¡ˆï¼š\n\né—®é¢˜ï¼š{q}\n\nç­”æ¡ˆï¼š"
                elif task_type == "information_extraction":
                    # è·å–é—®é¢˜æ–‡æœ¬ï¼ˆç”¨äºç¡®å®šæå–ç±»å‹ï¼‰å’Œå®é™…æ–‡æœ¬å†…å®¹
                    question_text = getattr(example, "question_text", None) or getattr(example, "text", "")
                    # ä»é—®é¢˜ä¸­æå–å®é™…æ–‡æœ¬å†…å®¹ï¼ˆå¼•å·å†…çš„éƒ¨åˆ†ï¼‰
                    import re
                    text_match = re.search(r"['""]([^'""]+)['""]", question_text)
                    if text_match:
                        actual_text = text_match.group(1)
                    else:
                        # å¦‚æœæ²¡æœ‰å¼•å·ï¼Œå°è¯•æå–"ä»"æˆ–"æå–"åé¢çš„å†…å®¹
                        text_match = re.search(r"(?:ä»|æå–)[^ï¼š:]*[ï¼š:]([^ï¼Œã€‚ï¼Ÿ]+)", question_text)
                        actual_text = text_match.group(1).strip() if text_match else question_text
                    
                    # ä½¿ç”¨é—®é¢˜æ–‡æœ¬ç”Ÿæˆæ¨¡æ¿ï¼Œä½¿ç”¨å®é™…æ–‡æœ¬å†…å®¹å¡«å……
                    extraction_template = self.prompt_optimizer._generate_extraction_template(question_text)
                    prompt = extraction_template.format(text=actual_text)
                elif task_type == "question_answering":
                    q = getattr(example, "question", "")
                    # ä½¿ç”¨ä¸“é—¨çš„æç¤ºè¯ç”Ÿæˆæ–¹æ³•ï¼Œç¡®ä¿æ­£ç¡®è§£æé—®é¢˜å’Œæ–‡æœ¬
                    prompt = self.prompt_optimizer._build_question_answering_prompt(q, [strategy])
                else:
                    prompt = optimized_prompt
                
                # è®¡ç®—æç¤ºè¯é•¿åº¦ï¼ˆç”¨äºä¼°ç®—æˆæœ¬ï¼‰
                prompt_tokens = len(prompt.split())  # ç®€å•ä¼°ç®—ï¼šæŒ‰è¯æ•°
                
                response = self.ollama_client.generate(prompt, max_tokens=50)
                
                # è®¡ç®—å“åº”æ—¶é—´
                response_time = time.time() - start_time
                
                # ä¼°ç®—æˆæœ¬ï¼ˆåŸºäºtokenæ•°ï¼Œç®€åŒ–è®¡ç®—ï¼‰
                # å‡è®¾ï¼šè¾“å…¥token $0.0001/1k tokens, è¾“å‡ºtoken $0.0002/1k tokens
                response_tokens = len(response.split())  # ç®€å•ä¼°ç®—
                estimated_cost = (prompt_tokens / 1000 * 0.0001) + (response_tokens / 1000 * 0.0002)
                
                # å°†å“åº”å°è£…ä¸º dspy.Predictionï¼Œå¹¶é™„åŠ æ—¶é—´å’Œæˆæœ¬ä¿¡æ¯
                if task_type == "math_reasoning":
                    answer = re.findall(r"[-+]?\d*\.?\d+", response)
                    pred = dspy.Prediction(answer=answer[0] if answer else response)
                elif task_type in ["text_classification", "sentiment_analysis", "sentiment_classification"]:
                    # è§„èŒƒåŒ–åˆ° 0/1ï¼Œé¿å…è¯„æµ‹é›†æ ‡ç­¾ä¸ä¸€è‡´å¯¼è‡´å‡†ç¡®ç‡ä¸º 0
                    resp_lower = str(response).lower().strip()
                    
                    # æ›´å®½æ¾çš„åŒ¹é…é€»è¾‘ï¼ˆé€‚ç”¨äºä¸­æ–‡å’Œè‹±æ–‡ï¼‰
                    # æ£€æŸ¥æ­£é¢å…³é”®è¯
                    positive_keywords = ["æ­£é¢", "ç§¯æ", "positive", "å¥½", "æ¨è", "å–œæ¬¢", "æ»¡æ„", "æ£’", "ä¼˜ç§€", "1"]
                    negative_keywords = ["è´Ÿé¢", "æ¶ˆæ", "negative", "ä¸å¥½", "å¤±æœ›", "å·®", "ç³Ÿç³•", "è®¨åŒ", "ä¸æ»¡", "0"]
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ­£é¢å…³é”®è¯
                    has_positive = any(kw in resp_lower for kw in positive_keywords)
                    has_negative = any(kw in resp_lower for kw in negative_keywords)
                    
                    if has_positive and not has_negative:
                        label = "1"
                    elif has_negative and not has_positive:
                        label = "0"
                    elif "ä¸­æ€§" in resp_lower or "neutral" in resp_lower:
                        label = "0"  # ä¸­æ€§æ˜ å°„ä¸º0ï¼Œä»¥é€‚é…äºŒåˆ†ç±»è¯„æµ‹æ ·æœ¬
                    else:
                        # å°è¯•æå–æ•°å­—ï¼ˆä¼˜å…ˆï¼‰
                        m = re.search(r"[01]", resp_lower)
                        if m:
                            label = m.group()
                        else:
                            # å¦‚æœéƒ½æ²¡æœ‰åŒ¹é…ï¼Œé»˜è®¤ä½¿ç”¨åŸå§‹å“åº”çš„å‰å‡ ä¸ªå­—ç¬¦
                            label = resp_lower[:10].strip() if len(resp_lower) > 10 else resp_lower
                    
                    pred = dspy.Prediction(sentiment=label)
                    # è°ƒè¯•ä¿¡æ¯
                    print(f"  ğŸ” æ–‡æœ¬åˆ†ç±»å“åº”å¤„ç†: åŸå§‹å“åº”='{response[:50]}...', è§„èŒƒåŒ–æ ‡ç­¾='{label}'")
                elif task_type == "information_extraction":
                    pred = dspy.Prediction(output=response)
                elif task_type == "question_answering":
                    # æ¸…ç†å“åº”ï¼Œæå–å®é™…ç­”æ¡ˆï¼ˆå‚è€ƒæ–‡æœ¬åˆ†ç±»å’Œä¿¡æ¯æŠ½å–ä»»åŠ¡çš„å¤„ç†æ–¹å¼ï¼‰
                    answer_clean = response.strip()
                    # ç§»é™¤å¯èƒ½çš„"ç­”æ¡ˆï¼š"å‰ç¼€
                    answer_clean = re.sub(r'^ç­”æ¡ˆ[:ï¼š]\s*', '', answer_clean)
                    # ç§»é™¤ç¤ºä¾‹æ ‡è®°
                    answer_clean = re.sub(r'ç¤ºä¾‹\d+\s*[:ï¼š]\s*', '', answer_clean)
                    
                    # ç§»é™¤"ä¾æ®"ã€"å› ä¸º"ç­‰è¯´æ˜æ€§æ–‡å­—ï¼ˆä¼˜å…ˆå¤„ç†ï¼Œé¿å…å¹²æ‰°åç»­æå–ï¼‰
                    answer_clean = re.sub(r'ä¾æ®[:ï¼š].*?[ã€‚\n]', '', answer_clean, flags=re.DOTALL)
                    answer_clean = re.sub(r'å› ä¸º.*?[ã€‚\n]', '', answer_clean, flags=re.DOTALL)
                    answer_clean = answer_clean.strip()
                    
                    # å¦‚æœå“åº”å¾ˆé•¿ï¼ˆå¯èƒ½æ˜¯å¤è¿°äº†æ•´ä¸ªæ–‡æœ¬ï¼‰ï¼Œå°è¯•æå–å…³é”®ä¿¡æ¯
                    if len(answer_clean) > 50:
                        # ä¼˜å…ˆæå–å¹´ä»½ï¼ˆå¦‚æœé—®é¢˜é—®çš„æ˜¯å¹´ä»½ï¼‰
                        year_match = re.search(r'(\d{4})å¹´', answer_clean)
                        if year_match:
                            answer_clean = year_match.group(1) + "å¹´"
                        else:
                            # æå–æ—¥æœŸ
                            date_match = re.search(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)', answer_clean)
                            if date_match:
                                answer_clean = date_match.group(1)
                            else:
                                # å°è¯•æå–ç¬¬ä¸€å¥
                                first_sentence = re.split(r'[ã€‚\n]', answer_clean)[0]
                                # å¦‚æœç¬¬ä¸€å¥è¿˜æ˜¯åŒ…å«æ•´ä¸ªæ–‡æœ¬çš„å¤è¿°ï¼Œå°è¯•æå–å…³é”®çŸ­è¯­
                                if len(first_sentence) > 50:
                                    # å°è¯•æå–å¼•å·ä¸­çš„å†…å®¹ï¼ˆå¯èƒ½æ˜¯ç­”æ¡ˆï¼‰
                                    quote_match = re.search(r"['""]([^'""]+)['""]", first_sentence)
                                    if quote_match:
                                        answer_clean = quote_match.group(1).strip()
                                    else:
                                        # å°è¯•æå–æœ€åä¸€ä¸ªçŸ­è¯­ï¼ˆå¯èƒ½æ˜¯ç­”æ¡ˆï¼‰
                                        # æŸ¥æ‰¾"æ˜¯"ã€"ä¸º"ã€"ï¼š"ç­‰å…³é”®è¯åé¢çš„å†…å®¹
                                        key_match = re.search(r'[æ˜¯ä¸ºï¼š]([^ï¼Œã€‚ï¼ï¼Ÿ\n]+)', first_sentence)
                                        if key_match:
                                            answer_clean = key_match.group(1).strip()
                                        else:
                                            # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œå–å‰30ä¸ªå­—ç¬¦
                                            answer_clean = answer_clean[:30].strip()
                                else:
                                    answer_clean = first_sentence.strip()
                    
                    # å¦‚æœç­”æ¡ˆä»ç„¶å¾ˆé•¿ï¼Œå°è¯•è¿›ä¸€æ­¥æå–å…³é”®æ•°å­—æˆ–çŸ­è¯­
                    if len(answer_clean) > 30:
                        # æå–å¹´ä»½
                        year_match = re.search(r'(\d{4})å¹´', answer_clean)
                        if year_match:
                            answer_clean = year_match.group(1) + "å¹´"
                        else:
                            # æå–æ—¥æœŸ
                            date_match = re.search(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)', answer_clean)
                            if date_match:
                                answer_clean = date_match.group(1)
                            else:
                                # æå–å¸¸è§åœ°ç‚¹åç§°ï¼ˆ2-4ä¸ªå­—ç¬¦çš„ä¸­æ–‡åœ°åï¼‰
                                location_match = re.search(r'([åŒ—äº¬ä¸Šæµ·å¹¿å·æ·±åœ³æ­å·å—äº¬æ­¦æ±‰æˆéƒ½é‡åº†è¥¿å®‰]{2,4})', answer_clean)
                                if location_match:
                                    answer_clean = location_match.group(1)
                                else:
                                    # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œå°è¯•æå–æœ€åä¸€ä¸ªçŸ­è¯­ï¼ˆå¯èƒ½æ˜¯ç­”æ¡ˆï¼‰
                                    parts = re.split(r'[ï¼Œã€‚ï¼ï¼Ÿ\n]', answer_clean)
                                    if parts:
                                        answer_clean = parts[-1].strip()
                    
                    pred = dspy.Prediction(answer=answer_clean)
                    # è°ƒè¯•ä¿¡æ¯
                    print(f"  ğŸ” é—®ç­”å“åº”å¤„ç†: åŸå§‹å“åº”='{response[:100]}...', æ¸…ç†åç­”æ¡ˆ='{answer_clean}'")
                else:
                    pred = dspy.Prediction(output=response)
                
                # å°†å“åº”æ—¶é—´å’Œæˆæœ¬é™„åŠ åˆ°é¢„æµ‹å¯¹è±¡
                pred.response_time = response_time
                pred.cost = estimated_cost
                
                return pred

            # ä¸´æ—¶è¯„æµ‹ï¼ŒåŒæ—¶æ”¶é›†å“åº”æ—¶é—´å’Œæˆæœ¬
            correct = 0
            total_response_time = 0.0
            total_cost = 0.0
            total_tested = 0  # è®°å½•å®é™…æµ‹è¯•çš„æ ·æœ¬æ•°
            
            print(f"  ğŸ“Š å¼€å§‹è¯„ä¼°ç­–ç•¥ '{strategy}'ï¼Œæµ‹è¯•æ ·æœ¬æ•°: {len(test_examples)}")
            
            for idx, ex in enumerate(test_examples):
                try:
                    total_tested += 1
                    print(f"  ğŸ“ æµ‹è¯•æ ·æœ¬ {idx + 1}/{len(test_examples)}")
                    pred = run_with_template(ex)
                    
                    # æ”¶é›†å“åº”æ—¶é—´å’Œæˆæœ¬
                    response_time = getattr(pred, 'response_time', 0.0)
                    cost = getattr(pred, 'cost', 0.0)
                    total_response_time += response_time if response_time > 0 else 0.0
                    total_cost += cost if cost > 0 else 0.0
                    
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºå“åº”æ—¶é—´å’Œæˆæœ¬
                    if task_type == "question_answering":
                        print(f"  â±ï¸ æ ·æœ¬ {idx + 1} å“åº”æ—¶é—´={response_time:.3f}s, æˆæœ¬=${cost:.6f}")
                    
                    if task_type == "information_extraction":
                        # è¯„ä¼°ï¼šç›®æ ‡ç­”æ¡ˆä¸­çš„å…³é”®çŸ­è¯­æ˜¯å¦éƒ½è¢«è¦†ç›–ï¼ˆå®½æ¾åŒ…å«ï¼‰
                        predicted = str(self.evaluator._get_prediction_output(pred))
                        # å°è¯•å¤šç§æ–¹å¼è·å–æœŸæœ›è¾“å‡º
                        expected = str(self.evaluator._get_example_output(ex))
                        if not expected or expected == "":
                            # å¦‚æœ_get_example_outputè¿”å›ç©ºï¼Œå°è¯•ç›´æ¥ä»exampleè·å–
                            expected = getattr(ex, 'output', '') or getattr(ex, 'answer', '') or str(ex)
                        
                        print(f"  ğŸ“Š ä¿¡æ¯æŠ½å–è¯„ä¼°: é¢„æµ‹='{predicted[:100]}...', æœŸæœ›='{expected[:100]}...'")
                        
                        # åŠ¨æ€æå–éœ€è¦æ£€æŸ¥çš„é”®ï¼ˆæ ¹æ®ç­”æ¡ˆæ ¼å¼ï¼‰
                        if "äº§å“" in expected and "ä»·æ ¼" in expected:
                            # äº§å“/ä»·æ ¼æ ¼å¼
                            keys = ["äº§å“", "ä»·æ ¼"]
                        else:
                            # é»˜è®¤äººå/åœ°ç‚¹/æ—¶é—´æ ¼å¼
                            keys = ["äººå", "åœ°ç‚¹", "æ—¶é—´"]
                        
                        expected_values = []
                        for k in keys:
                            # åŒ¹é…æ ¼å¼ï¼šé”®ï¼šå€¼ æˆ– é”®:å€¼ï¼ˆæ”¯æŒå¤šä¸ªå€¼ï¼Œç”¨é€—å·åˆ†éš”ï¼‰
                            # å…ˆå°è¯•åŒ¹é…æ•´ä¸ªé”®å€¼å¯¹ï¼ŒåŒ…æ‹¬å¯èƒ½çš„å¤šå€¼
                            pattern = rf"{k}[:ï¼š]([^ï¼Œ,ï¼›;ã€‚]+)"
                            matches = re.findall(pattern, expected)
                            for match in matches:
                                value = match.strip()
                                # å¦‚æœå€¼åŒ…å«é€—å·ï¼Œå¯èƒ½æ˜¯å¤šä¸ªå€¼ï¼Œéœ€è¦åˆ†å‰²
                                if ',' in value or 'ï¼Œ' in value:
                                    values = re.split(r'[,ï¼Œ]', value)
                                    expected_values.extend([v.strip() for v in values if v.strip()])
                                else:
                                    expected_values.append(value)
                        
                        # å»é‡ä½†ä¿æŒé¡ºåº
                        seen = set()
                        unique_expected_values = []
                        for v in expected_values:
                            if v and v not in seen:
                                seen.add(v)
                                unique_expected_values.append(v)
                        expected_values = unique_expected_values
                        
                        # æ£€æŸ¥é¢„æµ‹ç»“æœä¸­æ˜¯å¦åŒ…å«æ‰€æœ‰æœŸæœ›å€¼ï¼ˆä¸åŒºåˆ†é¡ºåºï¼‰
                        if expected_values:
                            # å¯¹äºæ¯ä¸ªæœŸæœ›å€¼ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨é¢„æµ‹ç»“æœä¸­å‡ºç°
                            matched_count = 0
                            for ev in expected_values:
                                if ev and ev in predicted:
                                    matched_count += 1
                            # å¦‚æœåŒ¹é…çš„æœŸæœ›å€¼æ•°é‡è¾¾åˆ°ä¸€å®šæ¯”ä¾‹ï¼ˆè‡³å°‘50%ï¼‰ï¼Œè®¤ä¸ºæ­£ç¡®
                            ok = matched_count >= max(1, len(expected_values) * 0.5)
                            print(f"  ğŸ“Š ä¿¡æ¯æŠ½å–åŒ¹é…: {matched_count}/{len(expected_values)} ä¸ªæœŸæœ›å€¼åŒ¹é…")
                        else:
                            # å¦‚æœæ²¡æœ‰æå–åˆ°æœŸæœ›å€¼ï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åŒ…å«åŒ¹é…
                            ok = expected.strip() in predicted.strip() or predicted.strip() in expected.strip()
                            print(f"  ğŸ“Š ä¿¡æ¯æŠ½å–ç®€å•åŒ¹é…: {ok}")
                        
                        if ok:
                            correct += 1
                            print(f"  âœ… æ ·æœ¬ {idx + 1} è¯„ä¼°æ­£ç¡®")
                        else:
                            print(f"  âŒ æ ·æœ¬ {idx + 1} è¯„ä¼°é”™è¯¯")
                    elif task_type == "question_answering":
                        # å‚è€ƒæ–‡æœ¬åˆ†ç±»å’Œä¿¡æ¯æŠ½å–ä»»åŠ¡çš„å¤„ç†æ–¹å¼
                        # è·å–é¢„æµ‹è¾“å‡º - ä¼˜å…ˆä»predçš„answerå­—æ®µè·å–
                        predicted = ""
                        if hasattr(pred, 'answer'):
                            predicted = str(pred.answer)
                        else:
                            predicted = str(self.evaluator._get_prediction_output(pred))
                        
                        # è·å–æœŸæœ›è¾“å‡º - ä¼˜å…ˆä»exçš„answerå­—æ®µè·å–
                        expected = ""
                        if hasattr(ex, 'answer'):
                            expected = str(ex.answer)
                        else:
                            expected = str(self.evaluator._get_example_output(ex))
                            if not expected or expected == "":
                                # å¦‚æœ_get_example_outputè¿”å›ç©ºï¼Œå°è¯•ç›´æ¥ä»exampleè·å–
                                expected = getattr(ex, 'answer', '') or getattr(ex, 'output', '') or ""
                        
                        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•ä»å­—å…¸å½¢å¼è·å–
                        if not expected or expected == "":
                            if isinstance(ex, dict):
                                expected = ex.get('answer', '') or ex.get('output', '')
                        
                        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•ä»æ‰€æœ‰å±æ€§ä¸­æŸ¥æ‰¾
                        if not expected or expected == "":
                            for attr in ['answer', 'output', 'expected_answer', 'correct_answer']:
                                if hasattr(ex, attr):
                                    value = getattr(ex, attr)
                                    if value and str(value) not in ['', 'None']:
                                        expected = str(value)
                                        break
                        
                        print(f"  ğŸ“Š é—®ç­”è¯„ä¼°: é¢„æµ‹='{predicted[:100]}...', æœŸæœ›='{expected[:100]}...'")
                        
                        # å¦‚æœé¢„æµ‹æˆ–æœŸæœ›ä¸ºç©ºï¼Œç›´æ¥åˆ¤å®šä¸ºé”™è¯¯
                        if not predicted or not expected:
                            print(f"  âŒ æ ·æœ¬ {idx + 1} è¯„ä¼°é”™è¯¯: é¢„æµ‹æˆ–æœŸæœ›ä¸ºç©º (é¢„æµ‹='{predicted}', æœŸæœ›='{expected}')")
                            ok = False
                        else:
                            # æ¸…ç†é¢„æµ‹å’ŒæœŸæœ›ç­”æ¡ˆï¼Œç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ ‡ç‚¹ï¼ˆå‚è€ƒä¿¡æ¯æŠ½å–ä»»åŠ¡çš„å¤„ç†æ–¹å¼ï¼‰
                            predicted_clean = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿ\s\n\t]', '', predicted.strip().lower())
                            expected_clean = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿ\s\n\t]', '', expected.strip().lower())
                            
                            print(f"  ğŸ“Š é—®ç­”æ¸…ç†å: é¢„æµ‹='{predicted_clean[:50]}', æœŸæœ›='{expected_clean[:50]}'")
                            
                            # æ–¹æ³•1ï¼šç›´æ¥åŒ…å«åŒ¹é…ï¼ˆæœ€ä¸¥æ ¼ï¼Œå‚è€ƒæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼‰
                            ok = expected_clean in predicted_clean
                            
                            # æ–¹æ³•2ï¼šå¦‚æœç›´æ¥åŒ…å«å¤±è´¥ï¼Œå°è¯•åå‘åŒ…å«ï¼ˆé¢„æµ‹è¾ƒçŸ­æ—¶ï¼‰
                            if not ok:
                                ok = predicted_clean in expected_clean
                            
                            # æ–¹æ³•3ï¼šå¦‚æœéƒ½å¤±è´¥ï¼Œå°è¯•æå–å…³é”®æ•°å­—æˆ–æ ¸å¿ƒè¯è¿›è¡ŒåŒ¹é…ï¼ˆå‚è€ƒä¿¡æ¯æŠ½å–ä»»åŠ¡çš„å¤šå€¼å¤„ç†ï¼‰
                            if not ok:
                                # æå–æ•°å­—ï¼ˆå¹´ä»½ã€æ—¥æœŸç­‰ï¼‰
                                expected_numbers = re.findall(r'\d+', expected_clean)
                                predicted_numbers = re.findall(r'\d+', predicted_clean)
                                if expected_numbers:
                                    # å¦‚æœæœŸæœ›ç­”æ¡ˆåŒ…å«æ•°å­—ï¼Œæ£€æŸ¥é¢„æµ‹æ˜¯å¦åŒ…å«ç›¸åŒæ•°å­—ï¼ˆè‡³å°‘åŒ¹é…ä¸€ä¸ªï¼‰
                                    matched_numbers = [num for num in expected_numbers if num in predicted_clean]
                                    if matched_numbers:
                                        ok = True
                                        print(f"  ğŸ“Š é—®ç­”æ•°å­—åŒ¹é…: æœŸæœ›æ•°å­—={expected_numbers}, é¢„æµ‹æ•°å­—={predicted_numbers}, åŒ¹é…={matched_numbers}")
                            
                            # æ–¹æ³•4ï¼šå¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆå‚è€ƒä¿¡æ¯æŠ½å–ä»»åŠ¡çš„50%é˜ˆå€¼ï¼‰
                            if not ok and expected_clean:
                                # è®¡ç®—åŒ¹é…çš„å­—ç¬¦æ•°
                                matched_chars = sum(1 for c in expected_clean if c in predicted_clean)
                                match_ratio = matched_chars / len(expected_clean) if expected_clean else 0
                                # ä½¿ç”¨50%é˜ˆå€¼ï¼Œä¸ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¿æŒä¸€è‡´
                                ok = match_ratio >= 0.5
                                if ok:
                                    print(f"  ğŸ“Š é—®ç­”éƒ¨åˆ†åŒ¹é…: {match_ratio:.2%}")
                            
                            # æ–¹æ³•5ï¼šå¦‚æœæœŸæœ›ç­”æ¡ˆå¾ˆçŸ­ï¼ˆ1-3ä¸ªå­—ç¬¦ï¼‰ï¼Œä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…
                            if not ok and len(expected_clean) <= 3:
                                # å¯¹äºçŸ­ç­”æ¡ˆï¼Œåªè¦é¢„æµ‹ä¸­åŒ…å«æœŸæœ›ç­”æ¡ˆçš„æ¯ä¸ªå­—ç¬¦ï¼Œå°±è®¤ä¸ºåŒ¹é…
                                ok = all(c in predicted_clean for c in expected_clean if c)
                                if ok:
                                    print(f"  ğŸ“Š é—®ç­”çŸ­ç­”æ¡ˆåŒ¹é…: æœŸæœ›='{expected_clean}', é¢„æµ‹åŒ…å«æ‰€æœ‰å­—ç¬¦")
                        
                        # å‚è€ƒæ–‡æœ¬åˆ†ç±»å’Œä¿¡æ¯æŠ½å–ä»»åŠ¡çš„è¯„ä¼°ç»“æœå¤„ç†
                        if ok:
                            correct += 1
                            print(f"  âœ… æ ·æœ¬ {idx + 1} è¯„ä¼°æ­£ç¡®")
                        else:
                            print(f"  âŒ æ ·æœ¬ {idx + 1} è¯„ä¼°é”™è¯¯ï¼Œé¢„æµ‹='{predicted[:50]}', æœŸæœ›='{expected[:50]}'")
                    elif task_type in ["text_classification", "sentiment_analysis", "sentiment_classification"]:
                        # æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼šä½¿ç”¨ä¸“é—¨çš„è¯„ä¼°é€»è¾‘
                        metric = self.evaluator.metric_strategies.get(task_type, self.evaluator.metric_strategies["default"])
                        ok = metric.evaluate(ex, pred)
                        # è°ƒè¯•ä¿¡æ¯
                        pred_output = getattr(pred, 'sentiment', None) or getattr(pred, 'output', None) or str(pred)
                        true_output = getattr(ex, 'sentiment', None) or getattr(ex, 'output', None) or str(ex)
                        print(f"  ğŸ“Š æ–‡æœ¬åˆ†ç±»è¯„ä¼°: é¢„æµ‹='{pred_output}', æœŸæœ›='{true_output}', æ­£ç¡®={ok}")
                        if ok:
                            correct += 1
                            print(f"  âœ… æ ·æœ¬ {idx + 1} è¯„ä¼°æ­£ç¡®")
                        else:
                            print(f"  âŒ æ ·æœ¬ {idx + 1} è¯„ä¼°é”™è¯¯")
                    else:
                        ok = self.evaluator.metric_strategies.get(
                            task_type, self.evaluator.metric_strategies["default"]
                        ).evaluate(ex, pred)
                        if ok:
                            correct += 1
                except Exception as e:
                    # æ‰“å°å¼‚å¸¸ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
                    print(f"  âŒ è¯„ä¼°å¼‚å¸¸ (æ ·æœ¬ {idx + 1}): {str(e)}")
                    import traceback
                    traceback.print_exc()
                    # å³ä½¿å‡ºç°å¼‚å¸¸ï¼Œä¹Ÿè¦è®°å½•æµ‹è¯•æ•°é‡ï¼Œä½†ä¸å¢åŠ correct
                    pass
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡ï¼ˆä½¿ç”¨å®é™…æµ‹è¯•çš„æ ·æœ¬æ•°ï¼‰
            actual_test_count = total_tested if total_tested > 0 else len(test_examples)
            if actual_test_count == 0:
                print(f"  âš ï¸ è­¦å‘Š: ç­–ç•¥ '{strategy}' æ²¡æœ‰æµ‹è¯•ä»»ä½•æ ·æœ¬ï¼test_exampleså¤§å°={len(test_examples)}")
                acc = 0.0
                avg_response_time = 0.0
                avg_cost = 0.0
            else:
                acc = correct / actual_test_count if actual_test_count > 0 else 0.0
                avg_response_time = total_response_time / actual_test_count if actual_test_count > 0 else 0.0
                avg_cost = total_cost / actual_test_count if actual_test_count > 0 else 0.0
            
            print(f"  ğŸ“Š ç­–ç•¥ '{strategy}' è¯„ä¼°ç»“æœ: æ­£ç¡®={correct}/{actual_test_count}, å‡†ç¡®ç‡={acc:.3f}, å“åº”æ—¶é—´={avg_response_time:.3f}s, æˆæœ¬=${avg_cost:.6f}")
            
            performance.append({
                "strategy": strategy,
                "accuracy": round(acc, 3),
                "response_time": round(avg_response_time, 3),
                "cost": round(avg_cost, 6),
                "template": optimized_prompt
            })

        # é€‰æ‹©æœ€ä½³ç­–ç•¥ï¼ˆç»¼åˆè€ƒè™‘å‡†ç¡®ç‡ã€å“åº”æ—¶é—´å’Œæˆæœ¬ï¼‰
        # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼šå‡†ç¡®ç‡æƒé‡0.5ï¼Œå“åº”æ—¶é—´æƒé‡0.25ï¼Œæˆæœ¬æƒé‡0.25
        if performance:
            # å½’ä¸€åŒ–å„æŒ‡æ ‡
            max_acc = max(p["accuracy"] for p in performance) if performance else 1.0
            min_time = min(p.get("response_time", 0) for p in performance) if performance else 0.0
            max_time = max(p.get("response_time", 0) for p in performance) if performance else 1.0
            min_cost = min(p.get("cost", 0) for p in performance) if performance else 0.0
            max_cost = max(p.get("cost", 0) for p in performance) if performance else 1.0
            
            for p in performance:
                # å½’ä¸€åŒ–å‡†ç¡®ç‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
                norm_acc = p["accuracy"] / max_acc if max_acc > 0 else 0
                # å½’ä¸€åŒ–å“åº”æ—¶é—´ï¼ˆè¶Šä½è¶Šå¥½ï¼Œæ‰€ä»¥å–åï¼‰
                norm_time = 1 - (p.get("response_time", 0) - min_time) / (max_time - min_time) if (max_time - min_time) > 0 else 1
                # å½’ä¸€åŒ–æˆæœ¬ï¼ˆè¶Šä½è¶Šå¥½ï¼Œæ‰€ä»¥å–åï¼‰
                norm_cost = 1 - (p.get("cost", 0) - min_cost) / (max_cost - min_cost) if (max_cost - min_cost) > 0 else 1
                
                # ç»¼åˆè¯„åˆ†
                p["composite_score"] = norm_acc * 0.5 + norm_time * 0.25 + norm_cost * 0.25
            
            # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
            performance.sort(key=lambda x: x.get("composite_score", x["accuracy"]), reverse=True)
        else:
            performance.sort(key=lambda x: x["accuracy"], reverse=True)
        
        # ç¡®ä¿beståŒ…å«æ‰€æœ‰å¿…è¦çš„å­—æ®µ
        if performance:
            best = performance[0]
            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
            if "accuracy" not in best:
                best["accuracy"] = 0.0
            if "response_time" not in best:
                best["response_time"] = 0.0
            if "cost" not in best:
                best["cost"] = 0.0
            if "template" not in best:
                best["template"] = ""
        else:
            best = {
                "strategy": candidate_strategies[0] if candidate_strategies else "zero_shot",
                "accuracy": 0.0,
                "response_time": 0.0,
                "cost": 0.0,
                "template": ""
            }
        
        print(f"  ğŸ“Š æœ€ä½³ç­–ç•¥: {best.get('strategy')}, å‡†ç¡®ç‡={best.get('accuracy')}, å“åº”æ—¶é—´={best.get('response_time')}, æˆæœ¬={best.get('cost')}")

        patterns_analysis = {
            "num_candidates": len(candidate_strategies),
            "task_type": task_type,
            "signals": ["é•¿åº¦ã€å…³é”®è¯ã€æ€ç»´é“¾æŒ‡å¼•"],
        }

        # æ·»åŠ ç­–ç•¥å¯¹æ¯”è¯¦æƒ…ï¼ˆåŒ…å«æ‰€æœ‰æŒ‡æ ‡ï¼‰
        strategy_comparison = []
        for p in performance:
            # å®‰å…¨è®¿é—®æ‰€æœ‰å­—æ®µï¼Œé¿å…KeyError
            strategy_comparison.append({
                "strategy": p.get("strategy", "unknown"),
                "accuracy": p.get("accuracy", 0.0),
                "response_time": p.get("response_time", 0.0),
                "cost": p.get("cost", 0.0),
                "template_preview": (p.get("template", "")[:100] + "..." if len(p.get("template", "")) > 100 else p.get("template", ""))
            })

        # ç¡®ä¿best_strategyå­˜åœ¨
        best_strategy_value = best.get("strategy", candidate_strategies[0] if candidate_strategies else "zero_shot")
        
        return {
            "patterns_analysis": patterns_analysis,
            "strategy_recommendations": [p.get("strategy", "unknown") for p in performance[:3] if p.get("strategy")],
            "best_strategy": best_strategy_value,
            "optimized_template": best.get("template", ""),
            "performance_estimate": {
                "accuracy": best.get("accuracy", 0.0),
                "response_time": best.get("response_time", 0.0),
                "cost": best.get("cost", 0.0)
            },
            "strategy_comparison": strategy_comparison  # æ·»åŠ ç­–ç•¥å¯¹æ¯”è¯¦æƒ…
        }

    def _init_ollama_client(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯"""
        try:
            model_name = config.get("model", "llama2")
            api_base = config.get("api_base", "http://localhost:11434")

            print(f"ğŸ”— è¿æ¥Ollama: {api_base}")
            print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}")

            # æ£€æŸ¥OllamaæœåŠ¡
            try:
                response = requests.get(f"{api_base}/api/tags", timeout=10)
                if response.status_code == 200:
                    models = response.json().get('models', [])
                    available_models = [model['name'] for model in models]
                    print(f"ğŸ“Š å¯ç”¨æ¨¡å‹: {available_models}")

                    if model_name not in available_models:
                        print(f"âŒ æ¨¡å‹ '{model_name}' ä¸å­˜åœ¨")
                        if available_models:
                            model_name = available_models[0]
                            print(f"ğŸ”„ ä½¿ç”¨æ¨¡å‹: {model_name}")
                        else:
                            raise Exception("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
                else:
                    raise Exception("OllamaæœåŠ¡ä¸å¯ç”¨")
            except Exception as e:
                print(f"âŒ Ollamaæ£€æŸ¥å¤±è´¥: {e}")
                return self._init_mock_client()

            # åˆ›å»ºOllamaå®¢æˆ·ç«¯
            client = DSPyOllamaClient(
                model=model_name,
                api_base=api_base,
                max_tokens=config.get("max_tokens", 512),
                temperature=config.get("temperature", 0.1)
            )

            # æµ‹è¯•è¿æ¥
            try:
                test_response = client.generate("æµ‹è¯•è¿æ¥ï¼Œè¯·å›å¤'OK'", max_tokens=10)
                print(f"âœ… è¿æ¥æµ‹è¯•: {test_response}")
            except Exception as e:
                print(f"âš ï¸ æµ‹è¯•å¤±è´¥: {e}")

            return client

        except Exception as e:
            print(f"âŒ å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return self._init_mock_client()

    def _init_mock_client(self):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿå®¢æˆ·ç«¯"""
        print("ğŸ”¶ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

        class MockClient:
            def __init__(self):
                self.history = []

            def generate(self, prompt: str, **kwargs) -> str:
                self.history.append(prompt)
                print(f"  ğŸ¤– æ¨¡æ‹Ÿè¯·æ±‚: {prompt[:100]}...")

                # æ™ºèƒ½æ¨¡æ‹Ÿå“åº” - é’ˆå¯¹ç”µå½±è¯„è®ºä¼˜åŒ–
                if "ç”µå½±" in prompt and "è¯„è®º" in prompt:
                    if "ç²¾å½©" in prompt or "å‡ºè‰²" in prompt or "æ¨è" in prompt:
                        return "æ­£é¢"
                    elif "ç³Ÿç³•" in prompt or "å·®åŠ²" in prompt or "æ— èŠ" in prompt:
                        return "è´Ÿé¢"
                    else:
                        return "ä¸­æ€§"
                elif "2+2" in prompt or "4+4" in prompt:
                    return "4"
                elif "3ä¹˜ä»¥5" in prompt or "3 * 5" in prompt:
                    return "15"
                elif "10é™¤ä»¥2" in prompt or "10/2" in prompt:
                    return "5"
                elif "6ä¹˜ä»¥7" in prompt or "6 * 7" in prompt:
                    return "42"
                elif "å¼€å¿ƒ" in prompt or "å¥½æ—¥å­" in prompt or "å¾ˆæ£’" in prompt:
                    return "1"
                elif "ä¼¤å¿ƒ" in prompt or "å¤©æ°”ä¸å¥½" in prompt:
                    return "0"
                elif "é—®é¢˜" in prompt:
                    # åŸºç¡€é—®ç­”å¯å‘å¼
                    # 1) å¹´ä»½/æ—¥æœŸé—®é¢˜ï¼šè¿”å›æ–‡æœ¬ä¸­å‡ºç°çš„å¹´ä»½æˆ–æ—¥æœŸ
                    year_match = re.search(r"(\d{4})å¹´", prompt)
                    if ("å“ªä¸€å¹´" in prompt or "ä½•æ—¶" in prompt or "ä»€ä¹ˆæ—¶å€™" in prompt or "æˆç«‹" in prompt) and year_match:
                        return year_match.group(1) + "å¹´"

                    date_match = re.search(r"(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)", prompt)
                    if ("å“ªä¸€å¤©" in prompt or "å…·ä½“æ—¥æœŸ" in prompt or "æ˜¯å“ªä¸€å¤©" in prompt) and date_match:
                        return date_match.group(1)

                    # 2) ä¸­å›½åœ°ç†å¸¸è¯†
                    if "ä¸­å›½" in prompt and "é¦–éƒ½" in prompt:
                        return "åŒ—äº¬"
                    if "ä¸­å›½" in prompt and "æœ€å¤§" in prompt and "åŸå¸‚" in prompt:
                        return "ä¸Šæµ·"

                    # 3) å¼•å·å†…å®¹åŒ¹é…ï¼šå¦‚æœé—®é¢˜æåˆ°â€œå«ä»€ä¹ˆåå­—â€ï¼Œè¿”å›æ–‡æœ¬ä¸­çš„äººå/å®ä½“
                    if any(keyword in prompt for keyword in ["å«ä»€ä¹ˆ", "å«ä»€ä¹ˆåå­—", "è°", "æ˜¯å“ªä½", "æ˜¯è°"]):
                        quote = re.search(r"[\"\'â€œâ€â€˜â€™]([^\"\'â€œâ€â€˜â€™]{1,12})[\"\'â€œâ€â€˜â€™]", prompt)
                        if quote:
                            return quote.group(1)

                    # 4) å¦‚æœå­˜åœ¨â€œæ˜¯â€æˆ–â€œä¸ºâ€åçš„çŸ­è¯­ï¼Œç›´æ¥è¿”å›
                    phrase = re.search(r"[æ˜¯ä¸ºï¼š]\s*([^ï¼Œã€‚ï¼ï¼Ÿ\n]{1,12})", prompt)
                    if phrase:
                        return phrase.group(1).strip()

                    # å…œåº•ï¼šè¿”å›æ–‡æœ¬ä¸­æœ€å¸¸å‡ºç°çš„å¹´ä»½æˆ–å…³é”®è¯
                    fallback_year = re.search(r"(\d{4})", prompt)
                    if fallback_year:
                        return fallback_year.group(1) + ("å¹´" if "å¹´" not in fallback_year.group(0) else "")

                    if "æµ‹è¯•" in prompt:
                        return "OK"

                    # é»˜è®¤è¿”å›ä¸€ä¸ªç®€çŸ­å›ç­”ï¼Œé¿å…é•¿æ–‡æœ¬
                    return "æ— æ³•ç¡®å®š"
                elif "æµ‹è¯•" in prompt:
                    return "OK"
                else:
                    return "æ¨¡æ‹Ÿå“åº”"

        return MockClient()

    def create_predictor(self, signature, task_type: str):
        """åˆ›å»ºé¢„æµ‹å™¨"""
        return DSPyBasicPredictor(signature, self.ollama_client, task_type)

    def run_complete_pipeline(self, tasks_config: Dict[str, Dict]) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è¯„æµ‹æµç¨‹"""
        results = {}

        for task_name, config in tasks_config.items():
            print(f"\nğŸ”§ å¤„ç†ä»»åŠ¡: {task_name}")

            try:
                # åˆ›å»ºé¢„æµ‹å™¨
                predictor = self.create_predictor(config['signature'], task_name)

                # è¯„æµ‹
                print("ğŸ“Š å¼€å§‹è¯„æµ‹...")
                task_results = self.evaluator.evaluate_task(
                    predictor,
                    config['test_examples'],
                    task_name
                )

                results[task_name] = {
                    'metrics': task_results,
                    'optimized': False,
                    'predictor_type': 'DSPyBasicPredictor'
                }

                print(f"âœ… {task_name} å®Œæˆ")

            except Exception as e:
                print(f"âŒ ä»»åŠ¡å¤±è´¥: {e}")
                results[task_name] = {'error': str(e)}

        return results


class DSPyCostTracker:
    """DSPyæˆæœ¬è¿½è¸ªå™¨"""

    def __init__(self):
        self.total_tokens = 0

    def track_call(self, tokens: int):
        self.total_tokens += tokens

    def get_cost(self) -> float:
        return self.total_tokens * 0.000002


def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""

    math_train = [
        dspy.Example(question="2+2=?", answer="4").with_inputs("question"),
        dspy.Example(question="3 * 5=?", answer="15").with_inputs("question"),
    ]

    math_test = [
        dspy.Example(question="4+4=?", answer="8").with_inputs("question"),
        dspy.Example(question="6 * 7=?", answer="42").with_inputs("question"),
    ]

    sentiment_train = [
        dspy.Example(text="æˆ‘å¾ˆå¼€å¿ƒ", sentiment="1").with_inputs("text"),
        dspy.Example(text="å¤©æ°”ä¸å¥½", sentiment="0").with_inputs("text"),
    ]

    sentiment_test = [
        dspy.Example(text="æˆ‘å¾ˆä¼¤å¿ƒ", sentiment="0").with_inputs("text"),
        dspy.Example(text="å¥½æ—¥å­", sentiment="1").with_inputs("text"),
    ]

    return {
        'math_train': math_train, 'math_test': math_test,
        'sentiment_train': sentiment_train, 'sentiment_test': sentiment_test
    }


def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹ - å®Œå…¨åŒ¹é…å›¾ç‰‡ç•Œé¢"""
    lm_config = {
        "model": "gemma3:1b",
        "api_base": "http://localhost:11434",
        "max_tokens": 512,
        "temperature": 0.1
    }

    optimizer = DSPyPipelineOptimizer(lm_config)

    # å®Œå…¨åŒ¹é…å›¾ç‰‡ä¸­çš„è¾“å…¥æ ¼å¼
    input_question = "è¿™éƒ¨ç”µå½±çš„è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼Ÿè¯„è®ºï¼šâ€˜è¿™éƒ¨ç”µå½±çš„å‰§æƒ…éå¸¸ç²¾å½©ï¼Œæ¼”å‘˜è¡¨æ¼”å‡ºè‰²ï¼Œå¼ºçƒˆæ¨èï¼â€™"

    try:
        print("\n" + "=" * 60)
        print("=" * 60)

        # æ¨¡æ‹Ÿå›¾ç‰‡ä¸­çš„æ‰€æœ‰é…ç½®
        optimization_result = optimizer.optimize_prompt(
            task_type="text_classification",
            input_question=input_question,
            strategies=["zero_shot", "few_shot"],
            model_type="local"
        )

        print("\nğŸ“Š ä¼˜åŒ–ç»“æœè¯¦æƒ…:")
        print(f"âœ… çŠ¶æ€: {optimization_result.get('status', 'N/A')}")
        print(f"ğŸ“ ä¼˜åŒ–åçš„æç¤ºè¯:\n{optimization_result.get('optimized_prompt', 'N/A')}")

        # æ­£ç¡®è¾“å‡ºä»»åŠ¡åˆ†æï¼ˆå­—å…¸æ ¼å¼ï¼‰
        task_analysis = optimization_result.get('task_analysis', {})
        print(f"ğŸ” ä»»åŠ¡åˆ†æ: {task_analysis}")

        complexity = optimization_result.get('complexity', 'N/A')
        print(f"âš¡ å¤æ‚åº¦: {complexity}")

        print(f"ğŸ”§ ä½¿ç”¨ç­–ç•¥: {optimization_result.get('strategies_used', [])}")
        print(f"ğŸ¤– æ¨¡å‹ç±»å‹: {optimization_result.get('model_type', 'N/A')}")

    except Exception as e:
        print(f"âŒ ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")

    # ç»§ç»­åŸæœ‰çš„å¤šä»»åŠ¡è¯„æµ‹
    class MathSignature:
        pass

    class SentimentSignature:
        pass

    sample_data = create_sample_data()

    tasks_config = {
        "math_reasoning": {
            "signature": MathSignature(),
            "train_examples": sample_data['math_train'],
            "test_examples": sample_data['math_test'],
        },
        "sentiment_analysis": {
            "signature": SentimentSignature(),
            "train_examples": sample_data['sentiment_train'],
            "test_examples": sample_data['sentiment_test'],
        },
        "text_classification": {
            "signature": SentimentSignature(),
            "train_examples": sample_data['sentiment_train'],
            "test_examples": sample_data['sentiment_test'],
        }
    }

    return optimizer.run_complete_pipeline(tasks_config)


def interactive_cli():
    """å‘½ä»¤è¡Œäº¤äº’ï¼šé€‰æ‹©ä»»åŠ¡ -> è¾“å…¥æ–‡æœ¬/é—®é¢˜ -> é¢„æµ‹/ä¸€è‡´æ€§æŠ•ç¥¨ã€‚"""
    print("\nğŸš€ DSPy å‘½ä»¤è¡Œäº¤äº’æ¨¡å¼ (Ctrl+C é€€å‡º)")
    lm_config = {"model": "gemma3:1b", "api_base": "http://localhost:11434", "max_tokens": 512, "temperature": 0.1}
    optimizer = DSPyPipelineOptimizer(lm_config)

    tasks = [
        ("text_classification", "æ–‡æœ¬åˆ†ç±»/æƒ…æ„Ÿåˆ†æ (text)"),
        ("sentiment_analysis", "æƒ…æ„Ÿåˆ†æ (text)"),
        ("math_reasoning", "æ•°å­¦æ¨ç† (question)")
    ]

    try:
        while True:
            print("\nå¯é€‰ä»»åŠ¡ï¼š")
            for idx, (_, name) in enumerate(tasks, 1):
                print(f"  {idx}. {name}")
            try:
                sel = int(input("é€‰æ‹©ä»»åŠ¡ç¼–å·: ").strip())
                task_type = tasks[sel - 1][0]
            except (ValueError, IndexError):
                print("è¾“å…¥æ— æ•ˆï¼Œè¯·é‡è¯•ã€‚")
                continue;

            text = input("è¾“å…¥æ–‡æœ¬/é—®é¢˜: ").strip()
            if task_type in ["text_classification", "sentiment_analysis"]:
                inputs = {"text": text}
            else:
                inputs = {"question": text}

            mode = input("ä½¿ç”¨ä¸€è‡´æ€§æŠ•ç¥¨? (y/N): ").strip().lower()
            if mode in ["y", "yes", "æ˜¯"]:
                try:
                    k = input("æ ·æœ¬æ•°(é»˜è®¤5): ").strip()
                    k = int(k) if k else 5
                except ValueError:
                    k = 5
                result = optimizer.self_consistent_answer(task_type, inputs, num_samples=max(3, k))
                print(f"\nä¸€è‡´æ€§ç»“æœ: {result['answer']}")
                print(f"æŠ•ç¥¨è¯¦æƒ…: {result['vote_detail']}")
            else:
                ans = optimizer.predict_once(task_type, inputs)
                print(f"\né¢„æµ‹ç»“æœ: {ans}")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ å·²é€€å‡º CLI æ¨¡å¼")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--cli":
        interactive_cli()
    else:
        print("ğŸš€ DSPy-Ollamaé›†æˆç³»ç»Ÿå¯åŠ¨")
        print("=" * 50)

        try:
            results = example_usage()

            print("\n" + "=" * 60)
            print("ğŸ“Š æœ€ç»ˆç»“æœ:")
            print("=" * 60)

            # è¾“å‡ºç»“æœ
            summary = {}
            for task_name, result in results.items():
                if 'metrics' in result:
                    metrics = result['metrics']
                    summary[task_name] = {
                        'accuracy': f"{metrics.get('accuracy', 0):.1%}",
                        'samples': metrics.get('samples_tested', 0)
                    }

                    print(f"\nğŸ¯ {task_name}:")
                    if 'detailed_results' in metrics:
                        for detail in metrics['detailed_results']:
                            status = "âœ…" if detail.get('correct', False) else "âŒ"
                            pred = detail.get('predicted', 'N/A')
                            expected = detail.get('expected', 'N/A')
                            print(f"  {status} é¢„æµ‹: {pred} | æœŸæœ›: {expected}")
                else:
                    summary[task_name] = {'error': result.get('error', 'æœªçŸ¥é”™è¯¯')}

            print("\n" + "=" * 60)
            print("ğŸ“ˆ æ€»ç»“:")
            for task, stats in summary.items():
                if 'error' in stats:
                    print(f"âŒ {task}: {stats['error']}")
                else:
                    print(f"âœ… {task}: å‡†ç¡®ç‡ {stats['accuracy']} (æ ·æœ¬: {stats['samples']})")

        except Exception as e:
            print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
            import traceback

            traceback.print_exc()