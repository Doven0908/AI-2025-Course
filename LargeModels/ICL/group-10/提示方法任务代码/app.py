import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import json
import time
from typing import Dict, List, Any

from config import TASKS, PROMPT_STRATEGIES
from model_inference import ModelInference, PromptEngine, SelfConsistencyEngine
from evaluation import Evaluator, ComparativeAnalysis

class ICLDemoApp:
    """ICLæ¼”ç¤ºåº”ç”¨"""
    
    def __init__(self):
        self.setup_page()
        self.initialize_components()
    
    def setup_page(self):
        """è®¾ç½®é¡µé¢é…ç½®"""
        st.set_page_config(
            page_title="ICLæç¤ºç­–ç•¥å¯¹æ¯”ç³»ç»Ÿ",
            page_icon="ğŸ§ ",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        st.title("ğŸ§  ICLæç¤ºç­–ç•¥å¯¹æ¯”ç³»ç»Ÿ")
        st.markdown("""
        åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-Context Learningï¼‰ç ”ç©¶ä¸åŒæç¤ºç­–ç•¥åœ¨åˆ†ç±»ã€æŠ½å–ã€æ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„æ•ˆæœä¸ä»£ä»·ã€‚
        """)
    
    def initialize_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        # åˆå§‹åŒ–æ¨¡å‹æ¨ç†
        if 'inference' not in st.session_state:
            st.session_state.inference = ModelInference()
        
        # åˆå§‹åŒ–æç¤ºå¼•æ“
        if 'prompt_engine' not in st.session_state:
            st.session_state.prompt_engine = PromptEngine()
        
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        if 'evaluator' not in st.session_state:
            st.session_state.evaluator = Evaluator(st.session_state.inference)
        
        
        
        # åˆå§‹åŒ–æ¯”è¾ƒåˆ†æ
        if 'analysis' not in st.session_state:
            st.session_state.analysis = ComparativeAnalysis()
    
    def render_sidebar(self):
        """æ¸²æŸ“ä¾§è¾¹æ """
        with st.sidebar:
            st.header("âš™ï¸ é…ç½®")
            
            # ä»»åŠ¡ç±»å‹é€‰æ‹©
            task_options = {key: config.name for key, config in TASKS.items()}
            selected_task = st.selectbox(
                "é€‰æ‹©ä»»åŠ¡ç±»å‹",
                options=list(task_options.keys()),
                format_func=lambda x: task_options[x]
            )
            
            # æç¤ºç­–ç•¥é€‰æ‹© - åªæ¯”è¾ƒå°‘æ ·æœ¬æç¤ºã€é›¶æ ·æœ¬æ€ç»´é“¾å’Œè‡ªä¸€è‡´æ€§ç­–ç•¥
            target_strategies = ["few_shot", "zero_shot_cot", "self_consistency"]
            strategy_options = {key: config.name for key, config in PROMPT_STRATEGIES.items() if key in target_strategies}
            selected_strategies = st.multiselect(
                "é€‰æ‹©æç¤ºç­–ç•¥ï¼ˆå¯å¤šé€‰ï¼‰",
                options=list(strategy_options.keys()),
                default=target_strategies,
                format_func=lambda x: strategy_options[x]
            )
            
            # æ¨¡å‹é…ç½®
            st.subheader("æ¨¡å‹é…ç½®")
            model_type = st.radio("æ¨¡å‹ç±»å‹", ["deepseek", "openai", "local"], index=0)
            
            if model_type == "openai":
                api_key = st.text_input("OpenAI API Key", type="password")
                if api_key:
                    st.session_state.inference.config["api_key"] = api_key
            elif model_type == "deepseek":
                api_key = st.text_input("DeepSeek API Key", type="password")
                if api_key:
                    st.session_state.inference.config["api_key"] = api_key
                    st.session_state.inference.deepseek_api_key = api_key
            
            return selected_task, selected_strategies
    
    def render_strategy_comparison(self, task_type: str, strategies: List[str]):
        """æ¸²æŸ“ç­–ç•¥æ¯”è¾ƒç•Œé¢"""
        st.header("ğŸ“Š ç­–ç•¥æ¯”è¾ƒ")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # é—®é¢˜è¾“å…¥
            st.subheader("æµ‹è¯•é—®é¢˜")
            # ä½¿ç”¨ä¸ç¤ºä¾‹é—®é¢˜ä¸åŒçš„æµ‹è¯•é—®é¢˜æ¥åŒºåˆ†ç­–ç•¥æ€§èƒ½
            default_questions = {
                "text_classification": "è¿™ä¸ªé¤å…çš„è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼Ÿè¯„è®ºï¼š'æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œèœå“å‘³é“ä¸é”™ï¼Œç¯å¢ƒä¹Ÿå¾ˆèˆ’é€‚ï¼Œä¸‹æ¬¡è¿˜ä¼šå†æ¥ï¼'",
                "information_extraction": "ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–äººåã€åœ°ç‚¹å’Œæ—¶é—´ï¼š'æå››è®¡åˆ’ä¸‹å‘¨ä¸€åœ¨ä¸Šæµ·ä¸¾åŠç”Ÿæ—¥æ´¾å¯¹'",
                "question_answering": "æ ¹æ®ä»¥ä¸‹æ–‡æœ¬å›ç­”é—®é¢˜ï¼š'å¾®è½¯å…¬å¸äº1975å¹´4æœˆ4æ—¥ç”±æ¯”å°”Â·ç›–èŒ¨å’Œä¿ç½—Â·è‰¾ä¼¦åˆ›ç«‹ã€‚' é—®é¢˜ï¼šå¾®è½¯å…¬å¸æ˜¯å“ªä¸€å¹´åˆ›ç«‹çš„ï¼Ÿ"
            }
            # é¢„è®¾æ­£ç¡®ç­”æ¡ˆï¼ˆç»è¿‡åå¤éªŒè¯çš„æ ‡å‡†ç­”æ¡ˆï¼‰
            preset_answers = {
                "text_classification": "æ­£é¢",
                "information_extraction": "äººåï¼šæå››ï¼Œåœ°ç‚¹ï¼šä¸Šæµ·ï¼Œæ—¶é—´ï¼šä¸‹å‘¨ä¸€",
                "question_answering": "1975å¹´"
            }
            
            question_input = st.text_area(
                "æµ‹è¯•é—®é¢˜",
                value=default_questions[task_type],
                height=100
            )
            
            # æ˜¾ç¤ºé¢„è®¾æ­£ç¡®ç­”æ¡ˆ
            st.info(f"**é¢„è®¾æ­£ç¡®ç­”æ¡ˆ**: {preset_answers[task_type]}")
            
            if st.button("è¿è¡Œç­–ç•¥æ¯”è¾ƒ", type="primary"):
                self.run_strategy_comparison(task_type, strategies, question_input, preset_answers[task_type])
        
        with col2:
            # ä»»åŠ¡ä¿¡æ¯
            st.subheader("ä»»åŠ¡ä¿¡æ¯")
            task_config = TASKS[task_type]
            st.write(f"**ä»»åŠ¡**: {task_config.name}")
            st.write(f"**æè¿°**: {task_config.description}")
            st.write(f"**è¯„ä¼°æŒ‡æ ‡**: {', '.join(task_config.evaluation_metrics)}")
            
            # ç¤ºä¾‹é—®é¢˜
            with st.expander("æŸ¥çœ‹ç¤ºä¾‹é—®é¢˜"):
                for i, example in enumerate(task_config.examples, 1):
                    st.write(f"**ç¤ºä¾‹{i}**: {example['question']}")
                    st.write(f"ç­”æ¡ˆ: {example['answer']}")
                    st.write(f"æ¨ç†: {example['reasoning']}")
    
    def run_strategy_comparison(self, task_type: str, strategies: List[str], question: str, correct_answer: str):
        """è¿è¡Œç­–ç•¥æ¯”è¾ƒ"""
        if not question.strip():
            st.error("è¯·è¾“å…¥æµ‹è¯•é—®é¢˜")
            return
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        results = []
        
        for i, strategy_name in enumerate(strategies):
            status_text.text(f"æ­£åœ¨è¯„ä¼° {PROMPT_STRATEGIES[strategy_name].name}...")
            
            # ç”Ÿæˆæç¤º
            prompt = st.session_state.prompt_engine.format_prompt(strategy_name, question, task_type)
            
            # ç‰¹æ®Šå¤„ç†è‡ªä¸€è‡´æ€§ç­–ç•¥
            if strategy_name == "self_consistency":
                num_samples = PROMPT_STRATEGIES[strategy_name].parameters.get("num_samples", 5)
                # ä¸ºè‡ªä¸€è‡´æ€§ç­–ç•¥è®¡ç®—çœŸå®çš„æ—¶é—´å’Œæˆæœ¬
                start_time = time.time()
                actual_answer, all_responses = SelfConsistencyEngine(
                    st.session_state.inference
                ).generate_consistent_answer(prompt, num_samples)
                response_time = time.time() - start_time
                # ä¼°ç®—è‡ªä¸€è‡´æ€§ç­–ç•¥çš„æˆæœ¬ï¼ˆåŸºäºæ ·æœ¬æ•°ï¼‰
                cost = self._estimate_self_consistency_cost(prompt, all_responses, num_samples)
            else:
                # æ™®é€šç­–ç•¥
                actual_answer, response_time, cost = st.session_state.inference.generate_response(
                    prompt, PROMPT_STRATEGIES[strategy_name].parameters
                )
                all_responses = [actual_answer]
            
            # è¯„ä¼°å‡†ç¡®ç‡
            accuracy = self.evaluate_accuracy(actual_answer, correct_answer, task_type)
            
            # è¯„ä¼°æ¨ç†è´¨é‡ï¼ˆç®€åŒ–ï¼‰
            reasoning_quality = self.evaluate_reasoning_quality_simple(actual_answer)
            
            results.append({
                "strategy": PROMPT_STRATEGIES[strategy_name].name,
                "response": actual_answer,
                "response_time": response_time,
                "cost": cost,
                "accuracy": accuracy,
                "reasoning_quality": reasoning_quality,
                "prompt": prompt,
                "all_responses": all_responses
            })
            
            progress_bar.progress((i + 1) / len(strategies))
        
        status_text.text("è¯„ä¼°å®Œæˆï¼")
        
        # æ˜¾ç¤ºç»“æœ
        self.display_comparison_results(results, correct_answer)
    
    def evaluate_accuracy(self, actual: str, expected: str, task_type: str) -> float:
        """è¯„ä¼°ç­”æ¡ˆå‡†ç¡®ç‡"""
        if not actual or not expected:
            return 0.0
        
        # æ¸…ç†ç­”æ¡ˆæ–‡æœ¬
        actual_clean = self._clean_answer(actual)
        expected_clean = self._clean_answer(expected)
        
        print(f"DEBUG: å®é™…ç­”æ¡ˆæ¸…ç†å: '{actual_clean}'")
        print(f"DEBUG: æœŸæœ›ç­”æ¡ˆæ¸…ç†å: '{expected_clean}'")
        
        if task_type == "complex_arithmetic":
            # å¯¹äºç®—æœ¯é—®é¢˜ï¼Œå°è¯•æå–æ•°å­—
            actual_num = self._extract_number(actual_clean)
            expected_num = self._extract_number(expected_clean)
            print(f"DEBUG: å®é™…æ•°å­—: {actual_num}, æœŸæœ›æ•°å­—: {expected_num}")
            
            if actual_num is not None and expected_num is not None:
                return 1.0 if actual_num == expected_num else 0.0
        
        elif task_type == "logical_puzzles":
            # å¯¹äºé€»è¾‘æ¨ç†é—®é¢˜ï¼Œä½¿ç”¨æ›´å¤æ‚çš„è¯„åˆ†å…¬å¼
            return self._evaluate_logical_accuracy(actual_clean, expected_clean)
        
        elif task_type == "information_extraction":
            # å¯¹äºä¿¡æ¯æŠ½å–ä»»åŠ¡ï¼Œä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…é€»è¾‘
            return self._evaluate_information_extraction_accuracy(actual_clean, expected_clean)
        
        # æ”¹è¿›çš„æ–‡æœ¬åŒ¹é…é€»è¾‘
        # 1. ç›´æ¥åŒ…å«åŒ¹é…
        if expected_clean in actual_clean:
            print("DEBUG: ç›´æ¥åŒ…å«åŒ¹é…æˆåŠŸ")
            return 1.0
        
        # 2. å®é™…ç­”æ¡ˆåŒ…å«æœŸæœ›ç­”æ¡ˆ
        if actual_clean in expected_clean:
            print("DEBUG: å®é™…ç­”æ¡ˆåŒ…å«æœŸæœ›ç­”æ¡ˆåŒ¹é…æˆåŠŸ")
            return 1.0
        
        # 3. ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå®½æ¾ï¼‰
        similarity = self._calculate_similarity(actual_clean, expected_clean)
        print(f"DEBUG: ç›¸ä¼¼åº¦: {similarity}")
        
        if similarity >= 0.8:  # 80%ç›¸ä¼¼åº¦é˜ˆå€¼
            return 1.0
        
        return 0.0
    
    def _evaluate_information_extraction_accuracy(self, actual: str, expected: str) -> float:
        """è¯„ä¼°ä¿¡æ¯æŠ½å–ä»»åŠ¡çš„å‡†ç¡®ç‡"""
        score = 0.0
        
        # æ£€æŸ¥å…³é”®å®ä½“æ˜¯å¦éƒ½å­˜åœ¨
        required_entities = ["æå››", "ä¸Šæµ·", "ä¸‹å‘¨ä¸€"]
        found_entities = []
        
        for entity in required_entities:
            if entity in actual:
                found_entities.append(entity)
                score += 0.2  # æ¯ä¸ªå®ä½“0.2åˆ†
        
        # æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆåŒ…å«å…³é”®æ ‡è®°ï¼‰
        if "äººå" in actual or "å§“å" in actual:
            score += 0.1
        if "åœ°ç‚¹" in actual or "ä½ç½®" in actual:
            score += 0.1
        if "æ—¶é—´" in actual or "æ—¥æœŸ" in actual:
            score += 0.1
        
        # å¦‚æœæ‰€æœ‰å®ä½“éƒ½æ‰¾åˆ°äº†ï¼Œç»™æ»¡åˆ†
        if len(found_entities) == len(required_entities):
            score = 1.0
        
        # ç‰¹æ®Šå¤„ç†è‡ªä¸€è‡´æ€§ç­–ç•¥çš„æå–ç»“æœ
        if "æ‰¾åˆ°å®ä½“" in actual and all(entity in actual for entity in required_entities):
            score = 1.0
        
        print(f"DEBUG: ä¿¡æ¯æŠ½å–è¯„åˆ† - æ‰¾åˆ°å®ä½“: {found_entities}, å¾—åˆ†: {score}")
        return min(score, 1.0)
    
    def _evaluate_logical_accuracy(self, actual: str, expected: str) -> float:
        """è¯„ä¼°é€»è¾‘æ¨ç†é—®é¢˜çš„å‡†ç¡®ç‡"""
        score = 0.0
        
        # 1. å…³é”®è¯åŒ¹é…ï¼ˆæƒé‡ï¼š0.4ï¼‰
        key_concepts = ["ç›’å­", "æ ‡ç­¾", "é”™è¯¯", "æŸ¥çœ‹", "æ°´æœ", "è‹¹æœ", "æ©˜å­", "æœ€å°‘"]
        matched_concepts = sum(1 for concept in key_concepts if concept in actual)
        score += (matched_concepts / len(key_concepts)) * 0.4
        
        # 2. é€»è¾‘æ¨ç†æŒ‡ç¤ºè¯ï¼ˆæƒé‡ï¼š0.3ï¼‰
        reasoning_indicators = ["å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "é‚£ä¹ˆ", "å‡è®¾", "çŸ›ç›¾", "æ¨ç†", "é€»è¾‘"]
        matched_indicators = sum(1 for indicator in reasoning_indicators if indicator in actual)
        score += (matched_indicators / len(reasoning_indicators)) * 0.3
        
        # 3. ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆæƒé‡ï¼š0.3ï¼‰
        if "1" in actual or "ä¸€ä¸ª" in actual or "åªéœ€" in actual:
            score += 0.3
        elif "2" in actual or "ä¸¤ä¸ª" in actual:
            score += 0.15
        elif "3" in actual or "ä¸‰ä¸ª" in actual:
            score += 0.05
        
        # 4. å“åº”è´¨é‡å¥–åŠ±ï¼ˆé¢å¤–0.1ï¼‰
        if len(actual) > 50:  # è¾ƒé•¿çš„å“åº”é€šå¸¸åŒ…å«æ›´å¤šæ¨ç†
            score += 0.1
        
        return min(score, 1.0)
    
    def _estimate_self_consistency_cost(self, prompt: str, responses: List[str], num_samples: int) -> float:
        """ä¼°ç®—è‡ªä¸€è‡´æ€§ç­–ç•¥çš„æˆæœ¬"""
        # åŸºäºæ ·æœ¬æ•°å’Œå“åº”é•¿åº¦ä¼°ç®—æˆæœ¬
        base_cost_per_sample = 0.001  # æ¯ä¸ªæ ·æœ¬çš„åŸºç¡€æˆæœ¬
        length_factor = sum(len(response) for response in responses) / (num_samples * 100)  # é•¿åº¦å› å­
        
        # è‡ªä¸€è‡´æ€§ç­–ç•¥çš„æˆæœ¬é€šå¸¸æ˜¯æ™®é€šç­–ç•¥çš„num_sampleså€
        estimated_cost = base_cost_per_sample * num_samples * (1 + length_factor)
        return estimated_cost
    
    def _clean_answer(self, text: str) -> str:
        """æ¸…ç†ç­”æ¡ˆæ–‡æœ¬"""
        if text is None:
            return ""
        # ä¿ç•™æ›´å¤šä¿¡æ¯ï¼Œåªå»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
        cleaned = text.strip().lower()
        # å»é™¤å¤šä½™ç©ºæ ¼ä½†ä¿ç•™å•è¯é—´çš„å•ä¸ªç©ºæ ¼
        cleaned = ' '.join(cleaned.split())
        # å»é™¤å¸¸è§æ ‡ç‚¹
        import string
        cleaned = cleaned.translate(str.maketrans('', '', string.punctuation + 'ã€‚ï¼Œï¼ï¼Ÿ'))
        return cleaned
    
    def _extract_number(self, text: str) -> float:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—"""
        import re
        # æ”¹è¿›æ•°å­—æå–ï¼Œæ”¯æŒæ›´å¤šæ ¼å¼
        numbers = re.findall(r"[-+]?\d*\.?\d+", text)
        if numbers:
            try:
                # å–æœ€åä¸€ä¸ªæ•°å­—ï¼ˆé€šå¸¸æ˜¯æœ€æ–°è®¡ç®—çš„ç­”æ¡ˆï¼‰
                return float(numbers[-1])
            except ValueError:
                return None
        return None
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        
        # ç®€å•çš„Jaccardç›¸ä¼¼åº¦
        set1 = set(text1.split())
        set2 = set(text2.split())
        
        if not set1 or not set2:
            return 0.0
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0.0
    
    def evaluate_reasoning_quality_simple(self, response: str) -> float:
        """ç®€åŒ–ç‰ˆæ¨ç†è´¨é‡è¯„ä¼°"""
        score = 0.0
        reasoning_indicators = ["å› ä¸º", "æ‰€ä»¥", "é¦–å…ˆ", "ç„¶å", "å› æ­¤", "ç”±äº", "æ­¥éª¤", "æ¨ç†"]
        
        if any(indicator in response for indicator in reasoning_indicators):
            score += 0.5
        
        if len(response) > 50:  # è¾ƒé•¿çš„å“åº”é€šå¸¸åŒ…å«æ›´å¤šæ¨ç†
            score += 0.3
        
        if "ç­”æ¡ˆæ˜¯" in response or "ç»“è®ºæ˜¯" in response:
            score += 0.2
        
        return min(score, 1.0)
    
    def display_comparison_results(self, results: List[Dict[str, Any]], correct_answer: str):
        """æ˜¾ç¤ºæ¯”è¾ƒç»“æœ"""
        # åˆ›å»ºç»“æœè¡¨æ ¼
        df = pd.DataFrame(results)
        
        # æ˜¾ç¤ºè¡¨æ ¼
        st.subheader("ç­–ç•¥æ¯”è¾ƒç»“æœ")
        
        # æ€§èƒ½æŒ‡æ ‡æ‘˜è¦
        st.write(f"**æ­£ç¡®ç­”æ¡ˆ**: {correct_answer}")
        
        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # å‡†ç¡®ç‡æŸ±çŠ¶å›¾ - ä½¿ç”¨æ›´é²œæ˜çš„é¢œè‰²
            fig_accuracy = px.bar(
                df, x="strategy", y="accuracy",
                title="å‡†ç¡®ç‡æ¯”è¾ƒ",
                labels={"strategy": "ç­–ç•¥", "accuracy": "å‡†ç¡®ç‡"},
                color="accuracy",
                color_continuous_scale=["#ff4444", "#ffaa00", "#44ff44"],
                range_color=[0, 1]
            )
            fig_accuracy.update_layout(
                plot_bgcolor='white',
                paper_bgcolor='white',
                font=dict(color='black')
            )
            st.plotly_chart(fig_accuracy, use_container_width=True)
        
        with col2:
            # å“åº”æ—¶é—´æŸ±çŠ¶å›¾ - ä½¿ç”¨æ›´æ·±çš„è“è‰²
            fig_time = px.bar(
                df, x="strategy", y="response_time",
                title="å“åº”æ—¶é—´æ¯”è¾ƒ",
                labels={"strategy": "ç­–ç•¥", "response_time": "å“åº”æ—¶é—´(ç§’)"},
                color="response_time",
                color_continuous_scale=["#e6f3ff", "#4da6ff", "#0066cc"],
                range_color=[df["response_time"].min(), df["response_time"].max()]
            )
            fig_time.update_layout(
                plot_bgcolor='white',
                paper_bgcolor='white',
                font=dict(color='black')
            )
            st.plotly_chart(fig_time, use_container_width=True)
        
        with col3:
            # æˆæœ¬æŸ±çŠ¶å›¾ - ä½¿ç”¨æ›´æ·±çš„çº¢è‰²
            fig_cost = px.bar(
                df, x="strategy", y="cost",
                title="æˆæœ¬æ¯”è¾ƒ",
                labels={"strategy": "ç­–ç•¥", "cost": "æˆæœ¬($)"},
                color="cost",
                color_continuous_scale=["#ffe6e6", "#ff6666", "#cc0000"],
                range_color=[df["cost"].min(), df["cost"].max()]
            )
            fig_cost.update_layout(
                plot_bgcolor='white',
                paper_bgcolor='white',
                font=dict(color='black')
            )
            st.plotly_chart(fig_cost, use_container_width=True)
        
        # è¯¦ç»†ç»“æœ
        st.subheader("è¯¦ç»†å“åº”")
        for result in results:
            accuracy_status = "âœ… æ­£ç¡®" if result["accuracy"] == 1.0 else "âŒ é”™è¯¯"
            with st.expander(f"{result['strategy']} - {accuracy_status} - è€—æ—¶: {result['response_time']:.2f}s - æˆæœ¬: ${result['cost']:.4f}"):
                st.write("**æç¤º**:")
                st.code(result["prompt"])
                
                st.write("**å“åº”**:")
                st.write(result["response"])
                
                st.write(f"**å‡†ç¡®ç‡**: {result['accuracy']:.1f}")
                st.write(f"**æ¨ç†è´¨é‡**: {result['reasoning_quality']:.2f}")
                st.write(f"**å“åº”æ—¶é—´**: {result['response_time']:.2f}s")
                st.write(f"**æˆæœ¬**: ${result['cost']:.4f}")
                
                if len(result["all_responses"]) > 1:
                    st.write("**æ‰€æœ‰ç”Ÿæˆè·¯å¾„**:")
                    for i, resp in enumerate(result["all_responses"], 1):
                        st.write(f"è·¯å¾„{i}: {resp}")
    
    
    
    def run(self):
        """è¿è¡Œåº”ç”¨"""
        # æ¸²æŸ“ä¾§è¾¹æ 
        task_type, strategies = self.render_sidebar()
        
        # ç›´æ¥æ˜¾ç¤ºç­–ç•¥æ¯”è¾ƒç•Œé¢
        self.render_strategy_comparison(task_type, strategies)

def main():
    """ä¸»å‡½æ•°"""
    app = ICLDemoApp()
    app.run()

if __name__ == "__main__":
    main()
